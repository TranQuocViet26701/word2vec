{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/TranQuocViet26701/word2vec/blob/main/BTL_WordEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Prerequisite Mathematical Concepts\n\nBefore diving into the Word2Vec models, we need to understand three fundamental mathematical concepts that form the backbone of these algorithms:\n\n1. **Conditional Probability** - Understanding how the probability of one event changes given information about another event\n2. **Maximum Likelihood Estimation (MLE)** - A method for finding the best parameters that explain our observed data\n3. **Softmax Function** - A way to convert raw scores into probabilities\n\nLet's explore each concept in detail with step-by-step explanations and examples.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 3.b Conditional Probability\n\n### What is Conditional Probability?\n\n**Conditional probability** answers the question: *\"What is the probability of event A happening, given that we already know event B has happened?\"*\n\nIn everyday language, we use conditional probability all the time:\n- \"What's the chance of rain **given that** the sky is cloudy?\"\n- \"What's the probability a student passes the exam **given that** they studied for 10 hours?\"\n\nIn Word2Vec, we ask similar questions:\n- \"What's the probability of seeing the word 'cat' **given that** the center word is 'pet'?\"\n- \"What's the probability of the center word being 'coffee' **given that** the surrounding words are 'I', 'drink', 'every', 'morning'?\"\n\n### The Formal Definition\n\nThe conditional probability of event $A$ given event $B$ is written as $P(A \\mid B)$ and is defined as:\n\n$$P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}$$\n\nWhere:\n- $P(A \\mid B)$ is read as \"the probability of A given B\"\n- $P(A \\cap B)$ is the probability that **both** A and B happen (the intersection)\n- $P(B)$ is the probability that B happens\n- **Important**: This formula only makes sense when $P(B) > 0$ (we can't condition on an impossible event)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Intuitive Understanding\n\nThink of conditional probability as **narrowing down your sample space**.\n\nImagine you have a bag with 10 balls:\n- 4 red balls\n- 6 blue balls\n- Among the red balls: 3 are large, 1 is small\n- Among the blue balls: 2 are large, 4 are small\n\n**Question 1**: What's the probability of picking a large ball?\n- Total large balls = 3 + 2 = 5\n- Total balls = 10\n- $P(\\text{Large}) = \\frac{5}{10} = 0.5$\n\n**Question 2**: What's the probability of picking a large ball, **given that** we already know it's red?\n- Now we only consider the red balls (our sample space is reduced!)\n- Red balls = 4\n- Large red balls = 3\n- $P(\\text{Large} \\mid \\text{Red}) = \\frac{3}{4} = 0.75$\n\nNotice how knowing the ball is red **changed** the probability from 0.5 to 0.75!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Step-by-Step Verification Using the Formula\n\nLet's verify our answer using the formal formula $P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}$\n\n**Given**:\n- Event $A$ = picking a large ball\n- Event $B$ = picking a red ball\n\n**Step 1**: Calculate $P(B)$ - the probability of picking a red ball\n$$P(\\text{Red}) = \\frac{\\text{Number of red balls}}{\\text{Total balls}} = \\frac{4}{10} = 0.4$$\n\n**Step 2**: Calculate $P(A \\cap B)$ - the probability of picking a ball that is BOTH large AND red\n$$P(\\text{Large} \\cap \\text{Red}) = \\frac{\\text{Number of large red balls}}{\\text{Total balls}} = \\frac{3}{10} = 0.3$$\n\n**Step 3**: Apply the conditional probability formula\n$$P(\\text{Large} \\mid \\text{Red}) = \\frac{P(\\text{Large} \\cap \\text{Red})}{P(\\text{Red})} = \\frac{0.3}{0.4} = \\frac{3}{4} = 0.75$$\n\nThis matches our intuitive answer of $0.75$.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### The Chain Rule of Probability\n\nFrom the conditional probability formula, we can derive an extremely useful result called the **Chain Rule**:\n\nStarting from:\n$$P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}$$\n\nWe can rearrange by multiplying both sides by $P(B)$:\n$$P(A \\cap B) = P(A \\mid B) \\times P(B)$$\n\nThis tells us: **The probability of A AND B happening equals the probability of B happening times the probability of A given B**.\n\n**Example**: What's the probability of drawing two aces in a row from a deck of cards (without replacement)?\n\nLet:\n- $A$ = second card is an ace\n- $B$ = first card is an ace\n\n**Step 1**: $P(B) = P(\\text{First card is ace}) = \\frac{4}{52} = \\frac{1}{13}$\n\n**Step 2**: $P(A \\mid B) = P(\\text{Second card is ace} \\mid \\text{First card was ace})$\n- After drawing one ace, there are 3 aces left and 51 cards total\n- $P(A \\mid B) = \\frac{3}{51} = \\frac{1}{17}$\n\n**Step 3**: Apply chain rule\n$$P(A \\cap B) = P(A \\mid B) \\times P(B) = \\frac{1}{17} \\times \\frac{1}{13} = \\frac{1}{221} \\approx 0.0045$$\n\n### Extension to Multiple Events\n\nThe chain rule extends to multiple events:\n$$P(A, B, C) = P(A) \\times P(B \\mid A) \\times P(C \\mid A, B)$$\n\nIn general, for events $X_1, X_2, \\ldots, X_n$:\n$$P(X_1, X_2, \\ldots, X_n) = P(X_1) \\times P(X_2 \\mid X_1) \\times P(X_3 \\mid X_1, X_2) \\times \\cdots \\times P(X_n \\mid X_1, \\ldots, X_{n-1})$$\n\nThis is crucial for modeling sequences of words in NLP!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Connection to Word2Vec\n\nIn Word2Vec, conditional probability is the core concept that drives how word embeddings are learned.\n\n**Skip-Gram Model** asks: Given a center word, what is the probability of each context word?\n$$P(w_{\\text{context}} \\mid w_{\\text{center}})$$\n\nFor example, given the sentence \"The **cat** sat on the mat\" with \"cat\" as the center word and window size 2:\n- $P(\\text{\"The\"} \\mid \\text{\"cat\"}) = ?$\n- $P(\\text{\"sat\"} \\mid \\text{\"cat\"}) = ?$\n- $P(\\text{\"on\"} \\mid \\text{\"cat\"}) = ?$\n\n**CBOW Model** asks: Given the context words, what is the probability of the center word?\n$$P(w_{\\text{center}} \\mid w_{\\text{context}_1}, w_{\\text{context}_2}, \\ldots)$$\n\nFor example: $P(\\text{\"cat\"} \\mid \\text{\"The\"}, \\text{\"sat\"}, \\text{\"on\"}) = ?$\n\n**Key Insight**: Word2Vec learns word vectors such that words appearing in similar contexts have similar vectors. The conditional probability framework allows us to:\n1. Formulate this as a prediction problem\n2. Use maximum likelihood (next section) to find the best word vectors\n3. Use softmax (later section) to convert similarity scores into probabilities\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 3.c Maximum Likelihood Estimation (MLE)\n\n### What is Maximum Likelihood Estimation?\n\n**Maximum Likelihood Estimation (MLE)** is a method for finding the best parameters of a statistical model. The idea is simple:\n\n> *\"Find the parameter values that make the observed data most probable.\"*\n\nIn other words, among all possible parameter values, we choose the ones that would most likely produce the data we actually observed.\n\n### Likelihood vs Probability: Understanding the Difference\n\nThis distinction is subtle but crucial:\n\n**Probability**: Given fixed parameters, what's the chance of observing certain data?\n- \"If a coin has 50% heads probability, what's the chance of getting 7 heads in 10 flips?\"\n- We fix $\\theta = 0.5$ and ask about $P(\\text{data})$\n\n**Likelihood**: Given observed data, how plausible are different parameter values?\n- \"I observed 7 heads in 10 flips. Is the coin's heads probability more likely 0.5 or 0.7?\"\n- We fix the data and ask about different values of $\\theta$\n\n**Mathematical notation**:\n- Probability: $P(\\text{Data} \\mid \\theta)$ - probability of data given fixed parameter $\\theta$\n- Likelihood: $\\mathcal{L}(\\theta \\mid \\text{Data})$ - likelihood of parameter $\\theta$ given fixed data\n\nNumerically, they are the same value! But conceptually:\n- In probability, $\\theta$ is fixed and Data varies\n- In likelihood, Data is fixed and $\\theta$ varies",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### The MLE Formula\n\nThe Maximum Likelihood Estimate is the parameter value $\\hat{\\theta}$ that maximizes the likelihood:\n\n$$\\hat{\\theta}_{MLE} = \\underset{\\theta}{\\arg\\max} \\; \\mathcal{L}(\\theta \\mid \\text{Data}) = \\underset{\\theta}{\\arg\\max} \\; P(\\text{Data} \\mid \\theta)$$\n\nWhere:\n- $\\hat{\\theta}_{MLE}$ is the MLE estimate (the \"best\" parameter value)\n- $\\underset{\\theta}{\\arg\\max}$ means \"find the value of $\\theta$ that maximizes...\"\n- $\\mathcal{L}(\\theta \\mid \\text{Data})$ is the likelihood function\n\n### Step-by-Step Example: The Coin Flip Problem\n\n**Problem**: You flip a coin 10 times and observe 7 heads and 3 tails. What is the most likely value for the coin's probability of heads?\n\n**Setup**:\n- Let $\\theta$ = probability of heads (what we want to find)\n- Observed data: 7 heads (H) and 3 tails (T) in 10 flips\n- Each flip is independent\n\n**Step 1: Write the likelihood function**\n\nFor a single coin flip:\n- $P(\\text{Heads} \\mid \\theta) = \\theta$\n- $P(\\text{Tails} \\mid \\theta) = 1 - \\theta$\n\nSince all flips are independent, we multiply the probabilities:\n$$\\mathcal{L}(\\theta) = P(\\text{7 heads, 3 tails} \\mid \\theta) = \\theta^7 \\times (1-\\theta)^3$$\n\n**Step 2: Try some values to build intuition**\n\nLet's calculate the likelihood for different values of $\\theta$:\n\n| $\\theta$ | $\\theta^7$ | $(1-\\theta)^3$ | $\\mathcal{L}(\\theta) = \\theta^7 \\times (1-\\theta)^3$ |\n|----------|------------|----------------|-----------------------------------------------------|\n| 0.5 | 0.0078 | 0.125 | 0.000977 |\n| 0.6 | 0.0280 | 0.064 | 0.001792 |\n| 0.7 | 0.0824 | 0.027 | 0.002224 |\n| 0.8 | 0.2097 | 0.008 | 0.001678 |\n\nNotice that $\\theta = 0.7$ gives the highest likelihood! This matches our intuition: 7 heads out of 10 suggests the probability should be around 70%.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "**Step 3: Find the exact maximum using calculus**\n\nTo find the exact value of $\\theta$ that maximizes the likelihood, we take the derivative and set it to zero.\n\nOur likelihood function is:\n$$\\mathcal{L}(\\theta) = \\theta^7 \\times (1-\\theta)^3$$\n\n**Step 3a**: Apply the product rule to find $\\frac{d\\mathcal{L}}{d\\theta}$\n\nProduct rule: $\\frac{d}{dx}[f(x) \\cdot g(x)] = f'(x) \\cdot g(x) + f(x) \\cdot g'(x)$\n\nLet $f(\\theta) = \\theta^7$ and $g(\\theta) = (1-\\theta)^3$\n\nThen:\n- $f'(\\theta) = 7\\theta^6$ (power rule)\n- $g'(\\theta) = 3(1-\\theta)^2 \\times (-1) = -3(1-\\theta)^2$ (chain rule)\n\nTherefore:\n$$\\frac{d\\mathcal{L}}{d\\theta} = 7\\theta^6 \\cdot (1-\\theta)^3 + \\theta^7 \\cdot (-3)(1-\\theta)^2$$\n\n**Step 3b**: Simplify\n\nFactor out common terms $\\theta^6(1-\\theta)^2$:\n$$\\frac{d\\mathcal{L}}{d\\theta} = \\theta^6(1-\\theta)^2 \\left[ 7(1-\\theta) - 3\\theta \\right]$$\n\nSimplify the bracket:\n$$= \\theta^6(1-\\theta)^2 \\left[ 7 - 7\\theta - 3\\theta \\right]$$\n$$= \\theta^6(1-\\theta)^2 \\left[ 7 - 10\\theta \\right]$$\n\n**Step 3c**: Set derivative to zero and solve\n\n$$\\theta^6(1-\\theta)^2(7 - 10\\theta) = 0$$\n\nThis equals zero when:\n- $\\theta = 0$ (trivial solution - coin never shows heads)\n- $\\theta = 1$ (trivial solution - coin always shows heads)\n- $7 - 10\\theta = 0 \\Rightarrow \\theta = 0.7$ (**this is our MLE!**)\n\n**Conclusion**: $\\hat{\\theta}_{MLE} = 0.7 = \\frac{7}{10} = \\frac{\\text{number of heads}}{\\text{total flips}}$\n\nThis is a beautiful result! The MLE for a coin's bias is simply the observed proportion of heads.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Why Use Log-Likelihood?\n\nIn practice, we almost always work with the **log-likelihood** instead of the likelihood directly:\n\n$$\\ell(\\theta) = \\log \\mathcal{L}(\\theta)$$\n\n**Why take the logarithm?**\n\n**Reason 1: Numerical Stability**\n\nWhen we have many data points, the likelihood becomes a product of many small probabilities:\n$$\\mathcal{L}(\\theta) = P(x_1|\\theta) \\times P(x_2|\\theta) \\times \\cdots \\times P(x_n|\\theta)$$\n\nFor example, with 1000 data points each with probability 0.01:\n$$\\mathcal{L}(\\theta) = 0.01^{1000} = 10^{-2000}$$\n\nThis number is so small that computers cannot represent it (underflow)!\n\n**Reason 2: Products Become Sums**\n\nThe logarithm transforms products into sums:\n$$\\log(a \\times b) = \\log(a) + \\log(b)$$\n\nSo:\n$$\\ell(\\theta) = \\log \\mathcal{L}(\\theta) = \\log P(x_1|\\theta) + \\log P(x_2|\\theta) + \\cdots + \\log P(x_n|\\theta)$$\n\n$$= \\sum_{i=1}^{n} \\log P(x_i|\\theta)$$\n\nSums are much easier to compute and differentiate than products!\n\n**Reason 3: Same Maximum**\n\nSince $\\log$ is a monotonically increasing function:\n- If $a > b$, then $\\log(a) > \\log(b)$\n\nTherefore, maximizing $\\mathcal{L}(\\theta)$ is equivalent to maximizing $\\log \\mathcal{L}(\\theta)$:\n$$\\underset{\\theta}{\\arg\\max} \\; \\mathcal{L}(\\theta) = \\underset{\\theta}{\\arg\\max} \\; \\log \\mathcal{L}(\\theta)$$",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Coin Flip Example with Log-Likelihood\n\nLet's redo our coin flip example using log-likelihood.\n\n**Original likelihood**:\n$$\\mathcal{L}(\\theta) = \\theta^7 \\times (1-\\theta)^3$$\n\n**Step 1: Take the logarithm**\n$$\\ell(\\theta) = \\log(\\theta^7 \\times (1-\\theta)^3)$$\n\nUsing $\\log(a \\times b) = \\log(a) + \\log(b)$:\n$$\\ell(\\theta) = \\log(\\theta^7) + \\log((1-\\theta)^3)$$\n\nUsing $\\log(a^n) = n \\times \\log(a)$:\n$$\\ell(\\theta) = 7\\log(\\theta) + 3\\log(1-\\theta)$$\n\n**Step 2: Find the derivative**\n$$\\frac{d\\ell}{d\\theta} = 7 \\times \\frac{1}{\\theta} + 3 \\times \\frac{-1}{1-\\theta}$$\n$$= \\frac{7}{\\theta} - \\frac{3}{1-\\theta}$$\n\n**Step 3: Set to zero and solve**\n$$\\frac{7}{\\theta} - \\frac{3}{1-\\theta} = 0$$\n\nMultiply both sides by $\\theta(1-\\theta)$:\n$$7(1-\\theta) - 3\\theta = 0$$\n$$7 - 7\\theta - 3\\theta = 0$$\n$$7 = 10\\theta$$\n$$\\theta = 0.7$$\n\n**Same answer**, but the calculation was simpler! No product rule needed.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### From MLE to Minimizing Loss: The Word2Vec Connection\n\nIn Word2Vec, we use MLE to find the best word vectors. Here's how it works:\n\n**The Setup**:\n- We have a text corpus with words $w_1, w_2, \\ldots, w_T$\n- We want to find word vectors (parameters $\\theta$) that maximize the probability of observing these word sequences\n\n**For Skip-Gram**: Given center word $w_c$, predict context words $w_o$\n\nThe likelihood of the entire corpus is:\n$$\\mathcal{L}(\\theta) = \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} P(w^{(t+j)} \\mid w^{(t)}; \\theta)$$\n\n**Taking the log**:\n$$\\ell(\\theta) = \\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P(w^{(t+j)} \\mid w^{(t)}; \\theta)$$\n\n**The Key Transformation: Maximization \u2192 Minimization**\n\nIn machine learning, we typically **minimize** a loss function rather than maximize likelihood. The conversion is simple:\n\n$$\\text{Maximize } \\ell(\\theta) \\quad \\Longleftrightarrow \\quad \\text{Minimize } -\\ell(\\theta)$$\n\nSo the **loss function** for Skip-Gram is:\n$$\\mathcal{L}_{loss} = -\\ell(\\theta) = -\\sum_{t=1}^{T} \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P(w^{(t+j)} \\mid w^{(t)}; \\theta)$$\n\nThis is called **Negative Log-Likelihood (NLL)** loss!\n\n**Why minimize negative log-likelihood?**\n1. Convention: ML frameworks are designed to minimize, not maximize\n2. Positive values: Loss is always non-negative (since $\\log P \\leq 0$ for $P \\leq 1$)\n3. Interpretation: Lower loss = higher likelihood = better model\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 3.d The Softmax Function\n\n### What is Softmax?\n\nThe **softmax function** converts a vector of arbitrary real numbers into a **probability distribution**. It takes a vector of \"scores\" (which can be any real numbers, positive or negative) and outputs a vector of probabilities that:\n1. Are all positive (between 0 and 1)\n2. Sum to exactly 1\n\nThis makes softmax perfect for multi-class classification where we need to predict the probability of each class.\n\n### The Softmax Formula\n\nGiven a vector of scores $\\mathbf{z} = (z_1, z_2, \\ldots, z_K)$ where $K$ is the number of classes, the softmax function outputs:\n\n$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$$\n\nOr for the entire vector:\n$$\\text{softmax}(\\mathbf{z}) = \\left( \\frac{e^{z_1}}{\\sum_{j=1}^{K} e^{z_j}}, \\frac{e^{z_2}}{\\sum_{j=1}^{K} e^{z_j}}, \\ldots, \\frac{e^{z_K}}{\\sum_{j=1}^{K} e^{z_j}} \\right)$$\n\nWhere:\n- $e \\approx 2.71828$ is Euler's number (base of natural logarithm)\n- $e^{z_i}$ is the exponential of $z_i$\n- The denominator $\\sum_{j=1}^{K} e^{z_j}$ is a **normalization constant** that ensures the outputs sum to 1",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Step-by-Step Numerical Example\n\nLet's compute softmax for the vector $\\mathbf{z} = (2, 1, 0.5)$\n\n**Step 1: Compute the exponential of each element**\n\n| $i$ | $z_i$ | $e^{z_i}$ | Calculation |\n|-----|-------|-----------|-------------|\n| 1 | 2 | $e^2 = 7.389$ | $2.71828^2 = 7.389$ |\n| 2 | 1 | $e^1 = 2.718$ | $2.71828^1 = 2.718$ |\n| 3 | 0.5 | $e^{0.5} = 1.649$ | $2.71828^{0.5} = 1.649$ |\n\n**Step 2: Calculate the sum of all exponentials (denominator)**\n\n$$\\sum_{j=1}^{3} e^{z_j} = e^2 + e^1 + e^{0.5} = 7.389 + 2.718 + 1.649 = 11.756$$\n\n**Step 3: Divide each exponential by the sum**\n\n| $i$ | $e^{z_i}$ | $\\text{softmax}(z_i) = \\frac{e^{z_i}}{11.756}$ | Probability |\n|-----|-----------|------------------------------------------------|-------------|\n| 1 | 7.389 | $\\frac{7.389}{11.756}$ | $0.628$ (62.8%) |\n| 2 | 2.718 | $\\frac{2.718}{11.756}$ | $0.231$ (23.1%) |\n| 3 | 1.649 | $\\frac{1.649}{11.756}$ | $0.140$ (14.0%) |\n\n**Step 4: Verify probabilities sum to 1**\n\n$$0.628 + 0.231 + 0.140 = 0.999 \\approx 1.0 \\; \\checkmark$$\n\n(The small error is due to rounding)\n\n**Interpretation**: The input with the highest score (2) gets the highest probability (62.8%), but the other classes still get non-zero probabilities.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Why Use the Exponential Function?\n\nYou might wonder: why $e^{z_i}$ instead of just $z_i$? There are several important reasons:\n\n**1. Always Positive**\n\nThe exponential function $e^x$ is always positive for any real number $x$:\n- $e^{-100} = 3.7 \\times 10^{-44}$ (very small, but positive!)\n- $e^{0} = 1$\n- $e^{100} = 2.7 \\times 10^{43}$ (very large)\n\nThis ensures all probabilities are positive, which is required!\n\n**2. Amplifies Differences**\n\nThe exponential function amplifies differences between scores:\n\n| $z_1$ | $z_2$ | Difference | $e^{z_1}$ | $e^{z_2}$ | Ratio $\\frac{e^{z_1}}{e^{z_2}}$ |\n|-------|-------|------------|-----------|-----------|-------------------------------|\n| 3 | 2 | 1 | 20.09 | 7.39 | 2.72 |\n| 5 | 2 | 3 | 148.4 | 7.39 | 20.09 |\n\nA score difference of 1 leads to a probability ratio of about 2.72 ($\\approx e$).\n\n**3. Differentiable**\n\nThe exponential function has a nice derivative: $\\frac{d}{dx}e^x = e^x$\n\nThis makes it easy to compute gradients for training neural networks with gradient descent.\n\n**4. Preserves Ranking**\n\nIf $z_1 > z_2$, then $e^{z_1} > e^{z_2}$, so $\\text{softmax}(z_1) > \\text{softmax}(z_2)$\n\nThe relative ordering is preserved.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Connection to Word2Vec\n\nIn Word2Vec, the softmax function is crucial for converting similarity scores between word vectors into probabilities.\n\n**Skip-Gram Model**: Given a center word $w_c$ with vector $\\mathbf{v}_c$, we want to compute the probability of each word $w_o$ being a context word.\n\n**Step 1**: Compute the \"score\" for each word using dot product\n\nThe dot product $\\mathbf{u}_o^\\top \\mathbf{v}_c$ measures how similar the context word vector $\\mathbf{u}_o$ is to the center word vector $\\mathbf{v}_c$:\n- High dot product \u2192 words are similar \u2192 high score\n- Low dot product \u2192 words are different \u2192 low score\n\n**Step 2**: Convert scores to probabilities using softmax\n\n$$P(w_o \\mid w_c) = \\text{softmax}(\\mathbf{u}_o^\\top \\mathbf{v}_c) = \\frac{\\exp(\\mathbf{u}_o^\\top \\mathbf{v}_c)}{\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)}$$\n\nWhere:\n- $\\mathbf{u}_o$ is the \"context\" vector for word $w_o$\n- $\\mathbf{v}_c$ is the \"center\" vector for word $w_c$\n- $\\mathcal{V}$ is the entire vocabulary\n- The denominator sums over ALL words in the vocabulary\n\n**Example Intuition**:\n\nConsider the center word \"coffee\" and three potential context words:\n\n| Context Word | Dot Product Score | After Softmax |\n|--------------|-------------------|---------------|\n| \"drink\" | 5.0 (high similarity) | 0.72 |\n| \"morning\" | 3.5 (moderate similarity) | 0.24 |\n| \"airplane\" | 0.5 (low similarity) | 0.04 |\n\nThe softmax converts these raw similarity scores into a valid probability distribution!\n\n**The Computational Challenge**:\n\nNotice that the denominator $\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)$ sums over the **entire vocabulary**. \n\nIf the vocabulary has 3 million words, this means:\n- 3 million dot products\n- 3 million exponentials\n- 3 million additions\n\n**For every single training example!** This is why approximate methods like Negative Sampling (covered earlier in this notebook) are used in practice.\n\n---",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAkVhrRPF2Me"
   },
   "source": [
    "# Self-Supervised word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07hmGxryFygC"
   },
   "source": [
    "Self-Supervised word2vec\n",
    "1. Gi\u1edbi thi\u1ec7u v\u1ec1 c\u00e1ch embed word th\u00e0nh vector hi\u1ec7u qu\u1ea3 h\u01a1n one-hot, s\u1eed d\u1ee5ng x\u00e1c su\u1ea5t, h\u1ecdc t\u1ef1 gi\u00e1m s\u00e1t.\n",
    "2. The Skip-Gram Model: Di\u1ec5n gi\u1ea3i l\u00fd thuy\u1ebft m\u00f4 h\u00ecnh, c\u00e1ch x\u00e2y d\u1ef1ng c\u00f4ng th\u1ee9c P t\u1eeb softmax, max likelihood, gi\u1ea3i th\u00edch h\u00e0m loss cho pha training (t\u1ea1i sao l\u1ea1i bi\u1ebfn \u0111\u1ed5i ra h\u00e0m L \u0111\u00f3 t\u1eeb max likelihood)\n",
    "3. The Continuous Bag of Words (CBOW) Model: Ng\u01b0\u1ee3c l\u1ea1i v\u1edbi Skip-Gram l\u00e0 v\u1edbi input context words t\u00ednh x\u00e1c su\u1ea5t sinh ra center word. Tr\u00ecnh b\u00e0y: Gi\u1ea3i th\u00edch c\u00f4ng th\u1ee9c x\u00e1c su\u1ea5t \u0111i\u1ec1u ki\u1ec7n c\u1ee7a m\u00f4 h\u00ecnh, max likelihood, c\u00f4ng th\u1ee9c pha training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fpJw-rS95iC"
   },
   "source": [
    "## Self-Supervised Word to Vector Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9ZMBcvl-VZF"
   },
   "source": [
    "To beyond the limitations of One-Hot method, there was an approach based on Self-Supervised learning with two architectures: The Skip-Gram model and the Continuous Bag-of-Words (CBOW) model [(Mikolov et al., 2013)](https://arxiv.org/pdf/1301.3781).\n",
    "\n",
    "Following this method, they do not treat words as atomic units \u2013 there is no notion of similarity between words. Instead, both CBOW and skip-gram learn distributed representations by leveraging the local context in which words appear. The CBOW model predicts a target word based on its surrounding words, effectively aggregating contextual information to infer meaning. Conversely, the skip-gram model predicts the surrounding context words given a single target word, aiming to learn word vectors that are informative enough to generate their typical neighbors in text. Together, these two approaches capture semantic and syntactic regularities by exploiting patterns that naturally occur in large corpora.\n",
    "\n",
    "This approach is grounded in the use of conditional probability, where models learn word representations by estimating the likelihood of a target word given its context or vice versa. Through these probability-based predictions, the embeddings capture meaningful semantic relationships directly from unlabeled text in the corpus - the reason why this method is self-supervised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNEF7Qha-Djw"
   },
   "source": [
    "### Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zz71h4bc-Ghp"
   },
   "source": [
    "### The Continuous Bag of Words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kH9vArdkfpm3"
   },
   "source": [
    "Why do we need to use approximate training instead of the full softmax?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3CgJIpGARdj"
   },
   "source": [
    "## Fully Softmax\n",
    "In general, according to (15.1.4)\n",
    "the log conditional probability\n",
    "involving any pair of the center word $w_c$ and\n",
    "the context word $w_o$ is\n",
    "\n",
    "$$\\log P(w_o \\mid w_c) =\\mathbf{u}_o^\\top \\mathbf{v}_c - \\log\\left(\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)\\right).$$\n",
    "\n",
    "where $\\mathcal{|V|}$ = vocabulary size so complexity for full softmax cost per training pair\n",
    "\n",
    "$$O(\\mathcal{|V|})$$\n",
    "\n",
    "\n",
    "For a typical Word2Vec with a vocabulary size of $|V| = 3{,}000{,}000$:\n",
    "\n",
    "- $\\approx 3{,}000{,}000$ dot products per update  \n",
    "- $\\approx 3{,}000{,}000$ gradient updates\n",
    "\n",
    "## Negative Sampling\n",
    "\n",
    "Replace the full softmax probability with a binary logistic classifier (Nagetive Sampling):\n",
    "\n",
    "- For each real pair (context, target):\n",
    "\n",
    "  - Predict $D = 1$\n",
    "\n",
    "- For k noise words sampled from a unigram distribution:\n",
    "\n",
    "  - Predict $D = 0$\n",
    "\n",
    "**Cost per step:**\n",
    "\n",
    "$$O(\\mathcal{k})$$\n",
    "\n",
    "where typically $k=5$ to $10$ $15$.\n",
    "\n",
    "So instead of 3M updates, you only do ~10 updates. Massive speedup $\\approx 500,000$x faster than full softmax.\n",
    "\n",
    "## Hierarchical Softmax\n",
    "\n",
    "Use a Huffman tree to reduce softmax from:\n",
    "\n",
    "$$O(\\mathcal{|V|})$$\n",
    "\n",
    "to:\n",
    "\n",
    "$$O(\\log_2{|V|})$$\n",
    "\n",
    "For a vocabulary size of 3M words:\n",
    "\n",
    "$$O(\\log_2{|3,000,000|}) \\approx 22$$\n",
    "\n",
    "So each update uses $\\log$ ~20 nodes instead of millions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEazcRt8Y8cQ"
   },
   "source": [
    "## Negative Sampling in Skip-Gram Model (SG)\n",
    "\n",
    "Negative sampling transform the multi-classification into binary classification.\n",
    "\n",
    "Given:\n",
    "- The center word $w_c$\n",
    "- The context word $w_o$\n",
    "\n",
    "The probability model will be:\n",
    "\n",
    "$$P(D=1\\mid w_c, w_o) = \\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c)$$\n",
    "\n",
    "Based on (15.1.5), given:\n",
    "- The text sequence of length $T$\n",
    "- The word at time step $t$ is $w^{(t)}$\n",
    "- The context window size be $m$\n",
    "\n",
    "The joint probability will be:\n",
    "\n",
    "$$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(D=1\\mid w^{(t)}, w^{(t+j)})$$\n",
    "\n",
    "The formula only considers those events that involve positive examples ($D = 1$). The joint probability is maximized to $1$ only if $v_c^\\top v_w \\to +\\infty$. In other words, all the word vectors are equal to infinity. We are expecting that adding negative examples ($D = 0$) will make more sense.\n",
    "\n",
    "Given:\n",
    "- $S$ is the event that a context word $w_o$ comes from the context window of a center word $w_c$\n",
    "- With predefined distribution $P(w)$ sample $K$ noise words, $N_k$ is the event that a noise word $w_k$ ($k = 1, ..., K$)\n",
    "\n",
    "So these events involving both the positive example and negative examples are $\\{S, N_1, ..., N_K \\}$.\n",
    "\n",
    "Negative sampling rewrites the conditional probability:\n",
    "\n",
    "$$P(w^{(t+j)} \\mid w^{(t)})$$\n",
    "$$= P_S \\prod_K P_k $$\n",
    "$$= P(D=1\\mid w^{(t)}, w^{(t+j)}) \\prod_{k=1,\\,w_k \\sim P(w)}^K P(D=0\\mid w^{(t)}, w_k) $$\n",
    "\n",
    "Given:\n",
    "- $i_t$ is index of a word $w^{(t)}$ at time step $t$\n",
    "- $h_k$ is index of a noise word $w_k$\n",
    "\n",
    "The logarithmic loss\n",
    "\n",
    "$$-\\log P(w^{(t+j)} \\mid w^{(t)})$$\n",
    "$$= -\\log P(D=1\\mid w^{(t)}, w^{(t+j)}) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log P(D=0\\mid w^{(t)}, w_k)$$\n",
    "\n",
    "because of classification binary so $P(D=0\\mid w^{(t)}, w_k) = 1 - P(D=1\\mid w^{(t)}, w_k)$, we have:\n",
    "\n",
    "$$= -\\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\left(1-\\sigma\\left(\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right)\\right)$$\n",
    "\n",
    "with $\\sigma(x) + \\sigma(-x) = 1$, We can infer:\n",
    "\n",
    "$$= -\\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\sigma\\left(-\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right)$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUAyeomkMCuq"
   },
   "source": [
    "## Negative Sampling in Conitnuous Bag-of-Word Model (CBOW)\n",
    "\n",
    "Given:\n",
    "- The center word $w_c$\n",
    "- The context vector $\\bar{\\mathbf{v}}_o = \\left(\\mathbf{v}_{o_1} + \\ldots + \\mathbf{v}_{o_{2m}} \\right)/(2m)$\n",
    "\n",
    "The probability model will be:\n",
    "\n",
    "$$P(D=1\\mid w_c, w_{o_1},..., w_{o_{2m}}) = \\sigma(\\mathbf{u}_o^\\top \\bar{\\mathbf{v}}_o)$$\n",
    "\n",
    "Based on (15.1.12), given:\n",
    "- The text sequence of length $T$\n",
    "- The word at time step $t$ is $w^{(t)}$\n",
    "- The context window size be $m$\n",
    "\n",
    "The joint probability will be:\n",
    "\n",
    "$$ \\prod_{t=1}^{T}  P(D=1 \\mid w^{(t)},  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)})$$\n",
    "\n",
    "Given:\n",
    "- $S$ is the event that a context word $w_o$ comes from the context window of a center word $w_c$\n",
    "- With predefined distribution $P(w)$ sample $K$ noise words, $N_k$ is the event that a noise word $w_k$ ($k = 1, ..., K$)\n",
    "\n",
    "Add negative examples\n",
    "\n",
    "$$P(w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}) \\mid w^{(t)})$$\n",
    "$$= P_S \\prod_K P_k $$\n",
    "$$= P(D=1\\mid w^{(t)},  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}) \\prod_{k=1,\\,w_k \\sim P(w)}^K P(D=0\\mid w_k, w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}) $$\n",
    "\n",
    "\n",
    "The logarithmic loss:\n",
    "\n",
    "$$-\\log P(w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}) \\mid w^{(t)})$$\n",
    "\n",
    "$$= -\\log P(D=1\\mid w^{(t)},  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log P(D=0\\mid w_k, w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)})$$\n",
    "\n",
    "\n",
    "$$= -\\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\left(1-\\sigma\\left(\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right)\\right)$$\n",
    "\n",
    "with $\\sigma(x) + \\sigma(-x) = 1$, We can infer (todo: cleaning):\n",
    "\n",
    "$$= -\\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\bar{\\mathbf{v}}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\sigma\\left(-\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right)$$\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
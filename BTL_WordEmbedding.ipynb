{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TranQuocViet26701/word2vec/blob/main/BTL_WordEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why do we need to use approximate training instead of the full softmax?"
      ],
      "metadata": {
        "id": "kH9vArdkfpm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fully Softmax\n",
        "In general, according to (15.1.4)\n",
        "the log conditional probability\n",
        "involving any pair of the center word $w_c$ and\n",
        "the context word $w_o$ is\n",
        "\n",
        "$$\\log P(w_o \\mid w_c) =\\mathbf{u}_o^\\top \\mathbf{v}_c - \\log\\left(\\sum_{i \\in \\mathcal{V}} \\exp(\\mathbf{u}_i^\\top \\mathbf{v}_c)\\right).$$\n",
        "\n",
        "where $\\mathcal{|V|}$ = vocabulary size so complexity for full softmax cost per training pair\n",
        "\n",
        "$$O(\\mathcal{|V|})$$\n",
        "\n",
        "\n",
        "For a typical Word2Vec with a vocabulary size of $|V| = 3{,}000{,}000$:\n",
        "\n",
        "- $\\approx 3{,}000{,}000$ dot products per update  \n",
        "- $\\approx 3{,}000{,}000$ gradient updates\n",
        "\n",
        "## Negative Sampling\n",
        "\n",
        "Replace the full softmax probability with a binary logistic classifier (Nagetive Sampling):\n",
        "\n",
        "- For each real pair (context, target):\n",
        "\n",
        "  - Predict $D = 1$\n",
        "\n",
        "- For k noise words sampled from a unigram distribution:\n",
        "\n",
        "  - Predict $D = 0$\n",
        "\n",
        "**Cost per step:**\n",
        "\n",
        "$$O(\\mathcal{k})$$\n",
        "\n",
        "where typically $k=5$ to $10$ $15$.\n",
        "\n",
        "So instead of 3M updates, you only do ~10 updates. Massive speedup $\\approx 500,000$x faster than full softmax.\n",
        "\n",
        "## Hierarchical Softmax\n",
        "\n",
        "Use a Huffman tree to reduce softmax from:\n",
        "\n",
        "$$O(\\mathcal{|V|})$$\n",
        "\n",
        "to:\n",
        "\n",
        "$$O(\\log_2{|V|})$$\n",
        "\n",
        "For a vocabulary size of 3M words:\n",
        "\n",
        "$$O(\\log_2{|3,000,000|}) \\approx 22$$\n",
        "\n",
        "So each update uses $\\log$ ~20 nodes instead of millions."
      ],
      "metadata": {
        "id": "M3CgJIpGARdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Negative Sampling in Skip-Gram Model (SG)\n",
        "\n",
        "Negative sampling transform the multi-classification into binary classification.\n",
        "\n",
        "Given:\n",
        "- The center word $w_c$\n",
        "- The context word $w_o$\n",
        "\n",
        "The probability model will be:\n",
        "\n",
        "$$P(D=1\\mid w_c, w_o) = \\sigma(\\mathbf{u}_o^\\top \\mathbf{v}_c)$$\n",
        "\n",
        "Based on (15.1.5), given:\n",
        "- The text sequence of length $T$\n",
        "- The word at time step $t$ is $w^{(t)}$\n",
        "- The context window size be $m$\n",
        "\n",
        "The joint probability will be:\n",
        "\n",
        "$$ \\prod_{t=1}^{T} \\prod_{-m \\leq j \\leq m,\\ j \\neq 0} P(D=1\\mid w^{(t)}, w^{(t+j)})$$\n",
        "\n",
        "The formula only considers those events that involve positive examples ($D = 1$). The joint probability is maximized to $1$ only if $v_c^\\top v_w \\to +\\infty$. In other words, all the word vectors are equal to infinity. We are expecting that adding negative examples ($D = 0$) will make more sense.\n",
        "\n",
        "Given:\n",
        "- $S$ is the event that a context word $w_o$ comes from the context window of a center word $w_c$\n",
        "- With predefined distribution $P(w)$ sample $K$ noise words, $N_k$ is the event that a noise word $w_k$ ($k = 1, ..., K$)\n",
        "\n",
        "So these events involving both the positive example and negative examples are $\\{S, N_1, ..., N_K \\}$.\n",
        "\n",
        "Negative sampling rewrites the conditional probability:\n",
        "\n",
        "$$P(w^{(t+j)} \\mid w^{(t)})$$\n",
        "$$= P_S \\prod_K P_k $$\n",
        "$$= P(D=1\\mid w^{(t)}, w^{(t+j)}) \\prod_{k=1,\\,w_k \\sim P(w)}^K P(D=0\\mid w^{(t)}, w_k) $$\n",
        "\n",
        "Given:\n",
        "- $i_t$ is index of a word $w^{(t)}$ at time step $t$\n",
        "- $h_k$ is index of a noise word $w_k$\n",
        "\n",
        "The logarithmic loss\n",
        "\n",
        "$$-\\log P(w^{(t+j)} \\mid w^{(t)})$$\n",
        "$$= -\\log P(D=1\\mid w^{(t)}, w^{(t+j)}) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log P(D=0\\mid w^{(t)}, w_k)$$\n",
        "\n",
        "because of classification binary so $P(D=0\\mid w^{(t)}, w_k) = 1 - P(D=1\\mid w^{(t)}, w_k)$, we have:\n",
        "\n",
        "$$= -\\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\left(1-\\sigma\\left(\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right)\\right)$$\n",
        "\n",
        "with $\\sigma(x) + \\sigma(-x) = 1$, We can infer:\n",
        "\n",
        "$$= -\\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\sigma\\left(-\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right)$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KEazcRt8Y8cQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Negative Sampling in Conitnuous Bag-of-Word Model (CBOW)\n",
        "\n",
        "Given:\n",
        "- The center word $w_c$\n",
        "- The context vector $\\bar{\\mathbf{v}}_o = \\left(\\mathbf{v}_{o_1} + \\ldots + \\mathbf{v}_{o_{2m}} \\right)/(2m)$\n",
        "\n",
        "The probability model will be:\n",
        "\n",
        "$$P(D=1\\mid w_c, w_{o_1},..., w_{o_{2m}}) = \\sigma(\\mathbf{u}_o^\\top \\bar{\\mathbf{v}}_o)$$\n",
        "\n",
        "Based on (15.1.12), given:\n",
        "- The text sequence of length $T$\n",
        "- The word at time step $t$ is $w^{(t)}$\n",
        "- The context window size be $m$\n",
        "\n",
        "The joint probability will be:\n",
        "\n",
        "$$ \\prod_{t=1}^{T}  P(D=1 \\mid w^{(t)},  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)})$$\n",
        "\n",
        "Given:\n",
        "- $S$ is the event that a context word $w_o$ comes from the context window of a center word $w_c$\n",
        "- With predefined distribution $P(w)$ sample $K$ noise words, $N_k$ is the event that a noise word $w_k$ ($k = 1, ..., K$)\n",
        "\n",
        "Add negative examples\n",
        "\n",
        "$$P(w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}) \\mid w^{(t)})$$\n",
        "$$= P_S \\prod_K P_k $$\n",
        "$$= P(D=1\\mid w^{(t)},  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}) \\prod_{k=1,\\,w_k \\sim P(w)}^K P(D=0\\mid w_k, w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}) $$\n",
        "\n",
        "\n",
        "The logarithmic loss:\n",
        "\n",
        "$$-\\log P(w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}) \\mid w^{(t)})$$\n",
        "\n",
        "$$= -\\log P(D=1\\mid w^{(t)},  w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)}) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log P(D=0\\mid w_k, w^{(t-m)}, \\ldots, w^{(t-1)}, w^{(t+1)}, \\ldots, w^{(t+m)})$$\n",
        "\n",
        "\n",
        "$$= -\\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\mathbf{v}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\left(1-\\sigma\\left(\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right)\\right)$$\n",
        "\n",
        "with $\\sigma(x) + \\sigma(-x) = 1$, We can infer (todo: cleaning):\n",
        "\n",
        "$$= -\\log\\, \\sigma\\left(\\mathbf{u}_{i_{t+j}}^\\top \\bar{\\mathbf{v}}_{i_t}\\right) - \\sum_{k=1,\\ w_k \\sim P(w)}^K \\log\\sigma\\left(-\\mathbf{u}_{h_k}^\\top \\mathbf{v}_{i_t}\\right)$$\n"
      ],
      "metadata": {
        "id": "jUAyeomkMCuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-Supervised word2vec"
      ],
      "metadata": {
        "id": "kAkVhrRPF2Me"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-Supervised word2vec\n",
        "1. Giới thiệu về cách embed word thành vector hiệu quả hơn one-hot, sử dụng xác suất, học tự giám sát.\n",
        "2. The Skip-Gram Model: Diễn giải lý thuyết mô hình, cách xây dựng công thức P từ softmax, max likelihood, giải thích hàm loss cho pha training (tại sao lại biến đổi ra hàm L đó từ max likelihood)\n",
        "3. The Continuous Bag of Words (CBOW) Model: Ngược lại với Skip-Gram là với input context words tính xác suất sinh ra center word. Trình bày: Giải thích công thức xác suất điều kiện của mô hình, max likelihood, công thức pha training."
      ],
      "metadata": {
        "id": "07hmGxryFygC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Self-Supervised Word to Vector Method"
      ],
      "metadata": {
        "id": "5fpJw-rS95iC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To beyond the limitations of One-Hot method, there was an approach based on Self-Supervised learning with two architectures: The Skip-Gram model and the Continuous Bag-of-Words (CBOW) model [(Mikolov et al., 2013)](https://arxiv.org/pdf/1301.3781).\n",
        "\n",
        "Following this method, they do not treat words as atomic units – there is no notion of similarity between words. Instead, both CBOW and skip-gram learn distributed representations by leveraging the local context in which words appear. The CBOW model predicts a target word based on its surrounding words, effectively aggregating contextual information to infer meaning. Conversely, the skip-gram model predicts the surrounding context words given a single target word, aiming to learn word vectors that are informative enough to generate their typical neighbors in text. Together, these two approaches capture semantic and syntactic regularities by exploiting patterns that naturally occur in large corpora.\n",
        "\n",
        "This approach is grounded in the use of conditional probability, where models learn word representations by estimating the likelihood of a target word given its context or vice versa. Through these probability-based predictions, the embeddings capture meaningful semantic relationships directly from unlabeled text in the corpus - the reason why this method is self-supervised."
      ],
      "metadata": {
        "id": "t9ZMBcvl-VZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skip-Gram Model"
      ],
      "metadata": {
        "id": "hNEF7Qha-Djw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Continuous Bag of Words Model"
      ],
      "metadata": {
        "id": "zz71h4bc-Ghp"
      }
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Implementation in PyTorch\n",
    "## A Beginner-Friendly Step-by-Step Guide\n",
    "\n",
    "This notebook implements **Word2Vec** (both **Skip-Gram** and **CBOW**) from scratch using PyTorch.\n",
    "\n",
    "It serves as a practical companion to the theory notebook (`BTL_WordEmbedding.ipynb`).\n",
    "\n",
    "---\n",
    "\n",
    "### What you'll learn:\n",
    "1. How to preprocess text for Word2Vec\n",
    "2. How to build training data for Skip-Gram and CBOW\n",
    "3. How to implement both models in PyTorch\n",
    "4. How to train and visualize word embeddings\n",
    "\n",
    "### Prerequisites:\n",
    "- Basic Python knowledge\n",
    "- Understanding of the theory (see `BTL_WordEmbedding.ipynb`)\n",
    "\n",
    "---\n",
    "\n",
    "### Table of Contents:\n",
    "1. [Setup & Imports](#1.-Setup-&-Imports)\n",
    "2. [Data Preparation](#2.-Data-Preparation)\n",
    "3. [Training Data Generation](#3.-Training-Data-Generation)\n",
    "4. [PyTorch Dataset & DataLoader](#4.-PyTorch-Dataset-&-DataLoader)\n",
    "5. [Skip-Gram Model](#5.-Skip-Gram-Model)\n",
    "6. [CBOW Model](#6.-CBOW-Model)\n",
    "7. [Training Loop](#7.-Training-Loop)\n",
    "8. [Evaluation & Visualization](#8.-Evaluation-&-Visualization)\n",
    "9. [Playing with Results](#9.-Playing-with-Results)\n",
    "10. [Summary & Next Steps](#10.-Summary-&-Next-Steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports\n",
    "\n",
    "First, let's import all the libraries we need. Each library has a specific purpose:\n",
    "\n",
    "| Library | Purpose |\n",
    "|---------|--------|\n",
    "| `torch` | Deep learning framework (PyTorch) |\n",
    "| `numpy` | Numerical computations |\n",
    "| `matplotlib` | Plotting and visualization |\n",
    "| `sklearn` | Machine learning utilities (t-SNE) |\n",
    "| `collections` | Data structures (Counter) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SECTION 1: IMPORTS AND SETUP\n",
    "# ============================================\n",
    "\n",
    "# Core Python libraries\n",
    "import random                          # For random number generation\n",
    "from collections import Counter        # For counting word frequencies\n",
    "\n",
    "# NumPy for numerical operations\n",
    "import numpy as np                     # Array operations\n",
    "\n",
    "# PyTorch for deep learning\n",
    "import torch                           # Core PyTorch\n",
    "import torch.nn as nn                  # Neural network modules\n",
    "import torch.nn.functional as F        # Activation functions, loss functions\n",
    "import torch.optim as optim            # Optimizers (Adam, SGD, etc.)\n",
    "from torch.utils.data import Dataset, DataLoader  # Data handling\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt        # Plotting\n",
    "from sklearn.manifold import TSNE      # Dimensionality reduction for visualization\n",
    "\n",
    "# Make plots look nicer\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SET RANDOM SEEDS FOR REPRODUCIBILITY\n",
    "# ============================================\n",
    "# Why? So we get the same results every time we run the notebook\n",
    "\n",
    "SEED = 42  # The answer to everything :)\n",
    "\n",
    "random.seed(SEED)           # Python's random\n",
    "np.random.seed(SEED)        # NumPy's random\n",
    "torch.manual_seed(SEED)     # PyTorch's random\n",
    "\n",
    "# If using GPU, set seed for that too\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(f\"Random seed set to: {SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CHECK DEVICE (CPU or GPU)\n",
    "# ============================================\n",
    "# GPU makes training faster, but CPU works fine for our small example\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SYSTEM INFORMATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU (this is fine for our small example!)\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 2. Data Preparation\n\nBefore we can train Word2Vec, we need to prepare our text data. This involves:\n\n1. **Creating a corpus** (collection of sentences)\n2. **Tokenization** (breaking sentences into words)\n3. **Building vocabulary** (mapping words to numbers)\n\nLet's go step by step!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 2.1 Our Custom Corpus\n\nWe'll use a simple, hand-crafted corpus about **animals and food**. \n\n**Why a simple corpus?**\n- Easy to understand what the model learns\n- Easy to debug when something goes wrong\n- We can predict what words should be similar:\n  - `\"cat\"` and `\"dog\"` should be similar (both are pets)\n  - `\"eat\"` and `\"drink\"` should be similar (both are actions)\n  - `\"fish\"` appears in multiple contexts (food for cats, swims in water)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 2.1 CREATE OUR CUSTOM CORPUS\n# ============================================\n# A corpus is simply a collection of text (sentences in our case)\n\ncorpus = [\n    \"the cat sat on the mat\",\n    \"the dog sat on the rug\",\n    \"the cat ate the fish\",\n    \"the dog ate the meat\",\n    \"the cat and the dog are friends\",\n    \"the fish swims in the water\",\n    \"the cat drinks milk\",\n    \"the dog drinks water\",\n    \"the cat chased the dog\",\n    \"the dog chased the cat\",\n    \"i love my cat\",\n    \"i love my dog\",\n    \"the cat is sleeping\",\n    \"the dog is sleeping\",\n    \"cats and dogs are pets\",\n]\n\n# Let's see our corpus\nprint(\"=\" * 60)\nprint(\"OUR CORPUS\")\nprint(\"=\" * 60)\nprint()\n\nfor i, sentence in enumerate(corpus, 1):\n    print(f\"  Sentence {i:2d}: \\\"{sentence}\\\"\")\n\nprint()\nprint(f\"Total sentences: {len(corpus)}\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2.2 Tokenization\n\n**Tokenization** = Breaking text into individual units called **tokens** (usually words)\n\nFor example:\n```\n\"the cat sat on the mat\"  →  [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n```\n\nOur tokenizer will:\n1. Convert to lowercase (already done in our corpus)\n2. Split by spaces\n3. (In real projects, you'd also handle punctuation, special characters, etc.)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 2.2 TOKENIZATION\n# ============================================\n\ndef tokenize(text):\n    \"\"\"\n    Simple tokenizer that splits text into words.\n    \n    Args:\n        text (str): Input sentence\n        \n    Returns:\n        list: List of words (tokens)\n    \n    Example:\n        >>> tokenize(\"the cat sat\")\n        ['the', 'cat', 'sat']\n    \"\"\"\n    # Convert to lowercase and split by spaces\n    return text.lower().split()\n\n# Let's test our tokenizer on one sentence\ntest_sentence = \"the cat sat on the mat\"\ntokens = tokenize(test_sentence)\n\nprint(\"=\" * 60)\nprint(\"TOKENIZATION EXAMPLE\")\nprint(\"=\" * 60)\nprint()\nprint(f\"  Input:  \\\"{test_sentence}\\\"\")\nprint(f\"  Output: {tokens}\")\nprint()\nprint(f\"  Number of tokens: {len(tokens)}\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# TOKENIZE THE ENTIRE CORPUS\n# ============================================\n\n# Apply tokenization to all sentences\ntokenized_corpus = [tokenize(sentence) for sentence in corpus]\n\nprint(\"=\" * 60)\nprint(\"TOKENIZED CORPUS\")\nprint(\"=\" * 60)\nprint()\n\nfor i, tokens in enumerate(tokenized_corpus, 1):\n    print(f\"  Sentence {i:2d}: {tokens}\")\n\nprint()\n\n# Flatten into a single list of all words\nall_words = [word for sentence in tokenized_corpus for word in sentence]\n\nprint(f\"Total words in corpus: {len(all_words)}\")\nprint(f\"First 20 words: {all_words[:20]}\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2.3 Building the Vocabulary\n\n**Vocabulary** = The set of all unique words in our corpus\n\nNeural networks work with **numbers**, not strings! So we need:\n\n1. **`word_to_idx`**: Dictionary mapping each word to a unique integer\n   - Example: `{\"the\": 0, \"cat\": 1, \"dog\": 2, ...}`\n\n2. **`idx_to_word`**: Dictionary mapping each integer back to its word\n   - Example: `{0: \"the\", 1: \"cat\", 2: \"dog\", ...}`\n\n3. **`vocab_size`**: Total number of unique words",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 2.3 BUILD THE VOCABULARY\n# ============================================\n\ndef build_vocabulary(tokenized_corpus):\n    \"\"\"\n    Build vocabulary from tokenized corpus.\n    \n    Args:\n        tokenized_corpus: List of tokenized sentences\n        \n    Returns:\n        word_to_idx: Dictionary mapping word -> index\n        idx_to_word: Dictionary mapping index -> word\n        word_counts: Counter object with word frequencies\n    \"\"\"\n    # Step 1: Count word frequencies\n    word_counts = Counter()\n    for sentence in tokenized_corpus:\n        word_counts.update(sentence)\n    \n    print(\"Step 1: Count word frequencies\")\n    print(f\"  Found {len(word_counts)} unique words\")\n    print(f\"  Most common: {word_counts.most_common(5)}\")\n    print()\n    \n    # Step 2: Sort words by frequency (most common first)\n    # This is optional but helps with consistency\n    sorted_words = sorted(word_counts.keys(), \n                          key=lambda x: (-word_counts[x], x))\n    \n    print(\"Step 2: Sort words by frequency\")\n    print(f\"  First 10 words: {sorted_words[:10]}\")\n    print()\n    \n    # Step 3: Create word-to-index mapping\n    word_to_idx = {word: idx for idx, word in enumerate(sorted_words)}\n    \n    print(\"Step 3: Create word_to_idx mapping\")\n    print(f\"  'the' -> {word_to_idx['the']}\")\n    print(f\"  'cat' -> {word_to_idx['cat']}\")\n    print(f\"  'dog' -> {word_to_idx['dog']}\")\n    print()\n    \n    # Step 4: Create index-to-word mapping (reverse of word_to_idx)\n    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n    \n    print(\"Step 4: Create idx_to_word mapping\")\n    print(f\"  0 -> '{idx_to_word[0]}'\")\n    print(f\"  1 -> '{idx_to_word[1]}'\")\n    print(f\"  2 -> '{idx_to_word[2]}'\")\n    \n    return word_to_idx, idx_to_word, word_counts\n\n# Build vocabulary\nprint(\"=\" * 60)\nprint(\"BUILDING VOCABULARY\")\nprint(\"=\" * 60)\nprint()\n\nword_to_idx, idx_to_word, word_counts = build_vocabulary(tokenized_corpus)\nvocab_size = len(word_to_idx)\n\nprint()\nprint(\"=\" * 60)\nprint(f\"VOCABULARY SIZE: {vocab_size} unique words\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# DISPLAY FULL VOCABULARY\n# ============================================\n\nprint(\"=\" * 60)\nprint(\"COMPLETE VOCABULARY\")\nprint(\"=\" * 60)\nprint()\nprint(f\"{'Index':<8} {'Word':<15} {'Frequency':<10}\")\nprint(\"-\" * 35)\n\nfor idx in range(vocab_size):\n    word = idx_to_word[idx]\n    freq = word_counts[word]\n    print(f\"{idx:<8} {word:<15} {freq:<10}\")\n\nprint(\"-\" * 35)\nprint(f\"Total: {vocab_size} words\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2.4 Visualizing Word Frequencies\n\nLet's visualize our vocabulary to better understand our data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 2.4 VISUALIZE WORD FREQUENCIES\n# ============================================\n\n# Get words and their frequencies\nwords = [idx_to_word[i] for i in range(vocab_size)]\nfrequencies = [word_counts[word] for word in words]\n\n# Create figure with 2 subplots\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot 1: Bar chart of word frequencies\nax1 = axes[0]\nbars = ax1.barh(words[::-1], frequencies[::-1], color='steelblue', edgecolor='navy')\nax1.set_xlabel('Frequency', fontsize=12)\nax1.set_ylabel('Word', fontsize=12)\nax1.set_title('Word Frequencies in Our Corpus', fontsize=14, fontweight='bold')\n\n# Add value labels on bars\nfor bar, freq in zip(bars, frequencies[::-1]):\n    ax1.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2, \n             str(freq), va='center', fontsize=9)\n\n# Plot 2: Pie chart of top words vs others\nax2 = axes[1]\ntop_n = 5\ntop_words = words[:top_n]\ntop_freqs = frequencies[:top_n]\nother_freq = sum(frequencies[top_n:])\n\npie_labels = top_words + ['Others']\npie_sizes = top_freqs + [other_freq]\ncolors = plt.cm.Blues(np.linspace(0.3, 0.9, len(pie_labels)))\n\nwedges, texts, autotexts = ax2.pie(pie_sizes, labels=pie_labels, autopct='%1.1f%%',\n                                    colors=colors, startangle=90)\nax2.set_title(f'Top {top_n} Words vs Others', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\n# Print summary statistics\nprint(\"=\" * 50)\nprint(\"VOCABULARY STATISTICS\")\nprint(\"=\" * 50)\nprint(f\"Total unique words: {vocab_size}\")\nprint(f\"Total word occurrences: {sum(frequencies)}\")\nprint(f\"Most common word: '{words[0]}' (appears {frequencies[0]} times)\")\nprint(f\"Least common words: {words[-3:]} (appear {frequencies[-1]} time each)\")\nprint(f\"Average frequency: {sum(frequencies)/len(frequencies):.2f}\")\nprint(\"=\" * 50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 3. Training Data Generation\n\nNow we need to create training data for our models:\n\n- **Skip-Gram**: Given a center word, predict context words\n- **CBOW**: Given context words, predict the center word\n\nBoth use the concept of a **context window**.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 3.1 Understanding the Context Window\n\nThe **context window** defines how many words on each side of the center word we consider as \"context\".\n\n**Example with window size = 2:**\n\n```\nSentence: \"the  cat  sat  on  the  mat\"\n                      ↑\n              center word = \"sat\"\n              \nContext window (m=2):\n    ← 2 words →  sat  ← 2 words →\n    \"the\" \"cat\"       \"on\" \"the\"\n    \nSo context words for \"sat\" are: [\"the\", \"cat\", \"on\", \"the\"]\n```\n\nLet's visualize this:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 3.1 VISUALIZE CONTEXT WINDOW\n# ============================================\n\ndef visualize_context_window(sentence, center_idx, window_size):\n    \"\"\"\n    Visualize the context window around a center word.\n    \n    Args:\n        sentence: String sentence\n        center_idx: Index of the center word\n        window_size: Number of words on each side to consider\n    \"\"\"\n    tokens = sentence.split()\n    \n    print(\"=\" * 70)\n    print(\"CONTEXT WINDOW VISUALIZATION\")\n    print(\"=\" * 70)\n    print()\n    print(f\"Sentence: \\\"{sentence}\\\"\")\n    print(f\"Window size (m): {window_size}\")\n    print(f\"Center word position: {center_idx}\")\n    print(f\"Center word: \\\"{tokens[center_idx]}\\\"\")\n    print()\n    \n    # Build visualization string\n    visual = \"\"\n    for i, token in enumerate(tokens):\n        if i == center_idx:\n            visual += f\"[{token.upper()}] \"  # Center word in brackets, uppercase\n        elif center_idx - window_size <= i <= center_idx + window_size and i != center_idx:\n            visual += f\"({token}) \"  # Context words in parentheses\n        else:\n            visual += f\" {token}  \"  # Other words\n    \n    print(\"Visualization:\")\n    print(f\"  {visual}\")\n    print()\n    print(\"  Legend: [CENTER] (context) other\")\n    print()\n    \n    # Get context words\n    context_words = []\n    context_positions = []\n    for offset in range(-window_size, window_size + 1):\n        if offset != 0:  # Skip center word\n            pos = center_idx + offset\n            if 0 <= pos < len(tokens):\n                context_words.append(tokens[pos])\n                context_positions.append(pos)\n    \n    print(f\"Context words: {context_words}\")\n    print(f\"Context positions: {context_positions}\")\n    print(\"=\" * 70)\n    \n    return context_words\n\n# Test with different center positions\nsentence = \"the cat sat on the mat\"\nprint()\nvisualize_context_window(sentence, center_idx=2, window_size=2)  # \"sat\" as center",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Let's see what happens at the edge of a sentence\nprint(\"\\\\nWhat happens at the BEGINNING of a sentence?\")\nprint(\"(We can only get words to the RIGHT)\")\nprint()\nvisualize_context_window(sentence, center_idx=0, window_size=2)  # \"the\" at start\n\nprint()\nprint(\"\\\\nWhat happens at the END of a sentence?\")\nprint(\"(We can only get words to the LEFT)\")\nprint()\nvisualize_context_window(sentence, center_idx=5, window_size=2)  # \"mat\" at end",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.2 Skip-Gram Training Pairs\n\n**Skip-Gram Goal**: Given a center word, predict each context word.\n\nFor each center word, we create **multiple training pairs** - one for each context word.\n\n```\nFor sentence \"the cat sat on the mat\" with center=\"sat\" and window=2:\n\nTraining pairs (center, context):\n    (sat, the)   ← predict \"the\" from \"sat\"\n    (sat, cat)   ← predict \"cat\" from \"sat\"\n    (sat, on)    ← predict \"on\" from \"sat\"\n    (sat, the)   ← predict \"the\" from \"sat\"\n```\n\nLet's generate these pairs:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 3.2 GENERATE SKIP-GRAM TRAINING PAIRS\n# ============================================\n\ndef generate_skipgram_pairs(tokenized_corpus, word_to_idx, window_size=2, verbose=False):\n    \"\"\"\n    Generate Skip-Gram training pairs.\n    \n    For each word in the corpus:\n        - Treat it as the center word\n        - Create pairs with each context word within the window\n    \n    Args:\n        tokenized_corpus: List of tokenized sentences\n        word_to_idx: Dictionary mapping words to indices\n        window_size: Number of words on each side to consider\n        verbose: If True, print details for first sentence\n        \n    Returns:\n        pairs: List of (center_idx, context_idx) tuples\n    \"\"\"\n    pairs = []\n    \n    for sent_idx, sentence in enumerate(tokenized_corpus):\n        # Print details for first sentence only\n        if verbose and sent_idx == 0:\n            print(f\"Processing sentence: {sentence}\")\n            print(\"-\" * 50)\n        \n        for center_pos, center_word in enumerate(sentence):\n            center_idx = word_to_idx[center_word]\n            \n            if verbose and sent_idx == 0:\n                print(f\"\\\\n  Center word: '{center_word}' (position {center_pos}, index {center_idx})\")\n            \n            # Get context words within the window\n            for offset in range(-window_size, window_size + 1):\n                if offset == 0:  # Skip the center word itself\n                    continue\n                \n                context_pos = center_pos + offset\n                \n                # Check if position is valid (within sentence bounds)\n                if 0 <= context_pos < len(sentence):\n                    context_word = sentence[context_pos]\n                    context_idx = word_to_idx[context_word]\n                    \n                    pairs.append((center_idx, context_idx))\n                    \n                    if verbose and sent_idx == 0:\n                        print(f\"    -> Context: '{context_word}' (position {context_pos}, index {context_idx})\")\n                        print(f\"       Pair: ({center_idx}, {context_idx}) = ('{center_word}', '{context_word}')\")\n    \n    return pairs\n\n# Set window size\nWINDOW_SIZE = 2\n\nprint(\"=\" * 70)\nprint(\"GENERATING SKIP-GRAM TRAINING PAIRS\")\nprint(\"=\" * 70)\nprint(f\"Window size: {WINDOW_SIZE}\")\nprint()\n\n# Generate pairs with verbose output for first sentence\nskipgram_pairs = generate_skipgram_pairs(tokenized_corpus, word_to_idx, WINDOW_SIZE, verbose=True)\n\nprint()\nprint(\"=\" * 70)\nprint(f\"Total Skip-Gram pairs generated: {len(skipgram_pairs)}\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# DISPLAY SAMPLE SKIP-GRAM PAIRS\n# ============================================\n\nprint(\"=\" * 70)\nprint(\"SAMPLE SKIP-GRAM PAIRS (First 30)\")\nprint(\"=\" * 70)\nprint()\nprint(f\"{'#':<4} {'Center Idx':<12} {'Context Idx':<12} {'Center Word':<15} {'Context Word':<15}\")\nprint(\"-\" * 70)\n\nfor i, (center_idx, context_idx) in enumerate(skipgram_pairs[:30]):\n    center_word = idx_to_word[center_idx]\n    context_word = idx_to_word[context_idx]\n    print(f\"{i+1:<4} {center_idx:<12} {context_idx:<12} '{center_word}'{'':13} '{context_word}'\")\n\nprint(\"-\" * 70)\nprint(f\"... and {len(skipgram_pairs) - 30} more pairs\")\nprint()\n\n# Show statistics\nprint(\"STATISTICS:\")\nprint(f\"  Total pairs: {len(skipgram_pairs)}\")\nprint(f\"  Unique center words used: {len(set(p[0] for p in skipgram_pairs))}\")\nprint(f\"  Unique context words used: {len(set(p[1] for p in skipgram_pairs))}\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3.3 CBOW Training Pairs\n\n**CBOW Goal**: Given context words, predict the center word.\n\nUnlike Skip-Gram, CBOW creates **one training sample** per center word, with **all context words** as input.\n\n```\nFor sentence \"the cat sat on the mat\" with center=\"sat\" and window=2:\n\nTraining pair (context_words, center):\n    ([the, cat, on, the], sat)  ← predict \"sat\" from all context words\n```\n\nThe context words are often averaged to create a single input vector.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 3.3 GENERATE CBOW TRAINING PAIRS\n# ============================================\n\ndef generate_cbow_pairs(tokenized_corpus, word_to_idx, window_size=2, verbose=False):\n    \"\"\"\n    Generate CBOW training pairs.\n    \n    For each word in the corpus:\n        - Treat it as the center word (target)\n        - Collect all context words within the window (input)\n    \n    Args:\n        tokenized_corpus: List of tokenized sentences\n        word_to_idx: Dictionary mapping words to indices\n        window_size: Number of words on each side to consider\n        verbose: If True, print details for first sentence\n        \n    Returns:\n        pairs: List of (context_indices_list, center_idx) tuples\n    \"\"\"\n    pairs = []\n    \n    for sent_idx, sentence in enumerate(tokenized_corpus):\n        if verbose and sent_idx == 0:\n            print(f\"Processing sentence: {sentence}\")\n            print(\"-\" * 60)\n        \n        for center_pos, center_word in enumerate(sentence):\n            center_idx = word_to_idx[center_word]\n            \n            # Collect ALL context words within the window\n            context_indices = []\n            context_words = []\n            \n            for offset in range(-window_size, window_size + 1):\n                if offset == 0:  # Skip center word\n                    continue\n                \n                context_pos = center_pos + offset\n                \n                if 0 <= context_pos < len(sentence):\n                    ctx_word = sentence[context_pos]\n                    ctx_idx = word_to_idx[ctx_word]\n                    context_indices.append(ctx_idx)\n                    context_words.append(ctx_word)\n            \n            # Only add if we have at least one context word\n            if context_indices:\n                pairs.append((context_indices, center_idx))\n                \n                if verbose and sent_idx == 0:\n                    print(f\"\\\\n  Center word (TARGET): '{center_word}' (index {center_idx})\")\n                    print(f\"  Context words (INPUT): {context_words}\")\n                    print(f\"  Context indices: {context_indices}\")\n                    print(f\"  Pair: ({context_indices}, {center_idx})\")\n    \n    return pairs\n\nprint(\"=\" * 70)\nprint(\"GENERATING CBOW TRAINING PAIRS\")\nprint(\"=\" * 70)\nprint(f\"Window size: {WINDOW_SIZE}\")\nprint()\n\n# Generate CBOW pairs\ncbow_pairs = generate_cbow_pairs(tokenized_corpus, word_to_idx, WINDOW_SIZE, verbose=True)\n\nprint()\nprint(\"=\" * 70)\nprint(f\"Total CBOW pairs generated: {len(cbow_pairs)}\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# DISPLAY SAMPLE CBOW PAIRS\n# ============================================\n\nprint(\"=\" * 80)\nprint(\"SAMPLE CBOW PAIRS (First 15)\")\nprint(\"=\" * 80)\nprint()\n\nfor i, (context_indices, center_idx) in enumerate(cbow_pairs[:15]):\n    center_word = idx_to_word[center_idx]\n    context_words = [idx_to_word[idx] for idx in context_indices]\n    \n    print(f\"Pair {i+1}:\")\n    print(f\"  Context (INPUT):  {context_indices} = {context_words}\")\n    print(f\"  Center (TARGET):  {center_idx} = '{center_word}'\")\n    print()\n\nprint(\"=\" * 80)\nprint()\n\n# Compare Skip-Gram vs CBOW\nprint(\"COMPARISON: Skip-Gram vs CBOW\")\nprint(\"-\" * 50)\nprint(f\"{'Aspect':<25} {'Skip-Gram':<20} {'CBOW':<20}\")\nprint(\"-\" * 50)\nprint(f\"{'Number of pairs':<25} {len(skipgram_pairs):<20} {len(cbow_pairs):<20}\")\nprint(f\"{'Input':<25} {'1 word (center)':<20} {'Multiple (context)':<20}\")\nprint(f\"{'Output':<25} {'1 word (context)':<20} {'1 word (center)':<20}\")\nprint(\"-\" * 50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 4. PyTorch Dataset & DataLoader\n\nPyTorch provides two important classes for data handling:\n\n1. **`Dataset`**: Stores your data samples and labels\n   - Must implement `__len__()` and `__getitem__()`\n\n2. **`DataLoader`**: Wraps a Dataset to enable:\n   - **Batching**: Group samples together\n   - **Shuffling**: Randomize order each epoch\n   - **Parallel loading**: Load data in background\n\nLet's create custom Datasets for Skip-Gram and CBOW.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 4.1 Skip-Gram Dataset\n\nFor Skip-Gram, each sample is simple:\n- **Input**: center word index (single integer)\n- **Target**: context word index (single integer)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 4.1 SKIP-GRAM DATASET\n# ============================================\n\nclass SkipGramDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset for Skip-Gram model.\n    \n    Each sample is a (center_word_idx, context_word_idx) pair.\n    \n    The model will learn to:\n        Given center_word_idx -> predict context_word_idx\n    \"\"\"\n    \n    def __init__(self, pairs):\n        \"\"\"\n        Args:\n            pairs: List of (center_idx, context_idx) tuples\n        \"\"\"\n        self.pairs = pairs\n        print(f\"SkipGramDataset created with {len(pairs)} samples\")\n    \n    def __len__(self):\n        \"\"\"\n        Return the total number of samples.\n        \n        This is required by PyTorch to know how many samples are in the dataset.\n        \"\"\"\n        return len(self.pairs)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        Return a single sample by index.\n        \n        Args:\n            idx: Index of the sample to return\n            \n        Returns:\n            center_idx: Tensor with center word index (input)\n            context_idx: Tensor with context word index (target)\n        \"\"\"\n        center_idx, context_idx = self.pairs[idx]\n        \n        # Convert to PyTorch tensors\n        # PyTorch models expect tensors, not Python integers\n        return torch.tensor(center_idx, dtype=torch.long), \\\n               torch.tensor(context_idx, dtype=torch.long)\n\n# Create Skip-Gram dataset\nprint(\"=\" * 60)\nprint(\"CREATING SKIP-GRAM DATASET\")\nprint(\"=\" * 60)\nprint()\n\nskipgram_dataset = SkipGramDataset(skipgram_pairs)\n\nprint()\nprint(\"Testing the dataset:\")\nprint(\"-\" * 40)\n\n# Test getting samples\nfor i in [0, 1, 2]:\n    center, context = skipgram_dataset[i]\n    print(f\"Sample {i}:\")\n    print(f\"  center_idx (input):  {center.item()} -> '{idx_to_word[center.item()]}'\")\n    print(f\"  context_idx (target): {context.item()} -> '{idx_to_word[context.item()]}'\")\n    print(f\"  Tensor types: center={center.dtype}, context={context.dtype}\")\n    print()\n\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.2 CBOW Dataset\n\nFor CBOW, each sample has:\n- **Input**: List of context word indices (variable length!)\n- **Target**: center word index (single integer)\n\n**Challenge**: Batching requires fixed-size tensors, but context lengths vary!\n\n**Solution**: Pad shorter contexts with a special value (-1) and track the actual length.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 4.2 CBOW DATASET\n# ============================================\n\nclass CBOWDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset for CBOW model.\n    \n    Each sample is a (context_word_indices, center_word_idx) pair.\n    \n    The model will learn to:\n        Given context_word_indices -> predict center_word_idx\n    \n    Note: We pad context to a fixed size for batching.\n    \"\"\"\n    \n    def __init__(self, pairs, max_context_size):\n        \"\"\"\n        Args:\n            pairs: List of (context_indices_list, center_idx) tuples\n            max_context_size: Maximum number of context words (2 * window_size)\n        \"\"\"\n        self.pairs = pairs\n        self.max_context_size = max_context_size\n        print(f\"CBOWDataset created with {len(pairs)} samples\")\n        print(f\"Maximum context size: {max_context_size}\")\n    \n    def __len__(self):\n        return len(self.pairs)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        Return a single sample by index.\n        \n        Returns:\n            context_indices: Tensor of shape (max_context_size,) - padded with -1\n            center_idx: Tensor with center word index\n            context_length: Tensor with actual number of context words\n        \"\"\"\n        context_indices, center_idx = self.pairs[idx]\n        actual_length = len(context_indices)\n        \n        # Pad context to fixed size with -1\n        # Example: [1, 2, 3] with max_size=4 -> [1, 2, 3, -1]\n        padded_context = context_indices + [-1] * (self.max_context_size - actual_length)\n        \n        return (\n            torch.tensor(padded_context, dtype=torch.long),      # Context (padded)\n            torch.tensor(center_idx, dtype=torch.long),          # Target\n            torch.tensor(actual_length, dtype=torch.long)        # Actual context length\n        )\n\n# Maximum context size = 2 * window_size (words on both sides)\nMAX_CONTEXT_SIZE = 2 * WINDOW_SIZE\n\nprint(\"=\" * 60)\nprint(\"CREATING CBOW DATASET\")\nprint(\"=\" * 60)\nprint()\n\ncbow_dataset = CBOWDataset(cbow_pairs, MAX_CONTEXT_SIZE)\n\nprint()\nprint(\"Testing the dataset:\")\nprint(\"-\" * 40)\n\n# Test getting samples - show different context lengths\nfor i in [0, 1, 5]:  # Different samples with potentially different lengths\n    context, center, length = cbow_dataset[i]\n    \n    # Get actual context words (non-padded)\n    actual_context = context[:length].tolist()\n    context_words = [idx_to_word[idx] for idx in actual_context]\n    \n    print(f\"Sample {i}:\")\n    print(f\"  Context tensor (padded):  {context.tolist()}\")\n    print(f\"  Actual context length:    {length.item()}\")\n    print(f\"  Actual context indices:   {actual_context}\")\n    print(f\"  Context words:            {context_words}\")\n    print(f\"  Center (target):          {center.item()} -> '{idx_to_word[center.item()]}'\")\n    print()\n\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4.3 Creating DataLoaders\n\nDataLoaders handle:\n- **Batching**: Group multiple samples into one tensor\n- **Shuffling**: Randomize order (good for training)\n- **Iteration**: Easy to loop over batches",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 4.3 CREATE DATALOADERS\n# ============================================\n\n# Hyperparameter: batch size\nBATCH_SIZE = 16  # Number of samples per batch\n\nprint(\"=\" * 60)\nprint(\"CREATING DATALOADERS\")\nprint(\"=\" * 60)\nprint()\nprint(f\"Batch size: {BATCH_SIZE}\")\nprint()\n\n# Create DataLoaders\nskipgram_loader = DataLoader(\n    skipgram_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,  # Shuffle for training\n    drop_last=False  # Keep incomplete last batch\n)\n\ncbow_loader = DataLoader(\n    cbow_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    drop_last=False\n)\n\nprint(f\"Skip-Gram DataLoader:\")\nprint(f\"  Total samples: {len(skipgram_dataset)}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Number of batches: {len(skipgram_loader)}\")\nprint()\n\nprint(f\"CBOW DataLoader:\")\nprint(f\"  Total samples: {len(cbow_dataset)}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Number of batches: {len(cbow_loader)}\")\nprint()\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# INSPECT A BATCH FROM EACH DATALOADER\n# ============================================\n\nprint(\"=\" * 70)\nprint(\"EXAMPLE BATCH FROM SKIP-GRAM DATALOADER\")\nprint(\"=\" * 70)\nprint()\n\n# Get one batch from Skip-Gram\nfor batch_center, batch_context in skipgram_loader:\n    print(f\"Batch center words (input):\")\n    print(f\"  Shape: {batch_center.shape}\")\n    print(f\"  Values: {batch_center.tolist()}\")\n    print(f\"  Words: {[idx_to_word[i] for i in batch_center.tolist()]}\")\n    print()\n    print(f\"Batch context words (target):\")\n    print(f\"  Shape: {batch_context.shape}\")\n    print(f\"  Values: {batch_context.tolist()}\")\n    print(f\"  Words: {[idx_to_word[i] for i in batch_context.tolist()]}\")\n    break  # Only show first batch\n\nprint()\nprint(\"=\" * 70)\nprint(\"EXAMPLE BATCH FROM CBOW DATALOADER\")\nprint(\"=\" * 70)\nprint()\n\n# Get one batch from CBOW\nfor batch_context, batch_center, batch_lengths in cbow_loader:\n    print(f\"Batch context words (input):\")\n    print(f\"  Shape: {batch_context.shape} (batch_size x max_context_size)\")\n    print(f\"  First 3 samples: {batch_context[:3].tolist()}\")\n    print()\n    print(f\"Batch lengths:\")\n    print(f\"  Shape: {batch_lengths.shape}\")\n    print(f\"  Values: {batch_lengths.tolist()}\")\n    print()\n    print(f\"Batch center words (target):\")\n    print(f\"  Shape: {batch_center.shape}\")\n    print(f\"  Values: {batch_center.tolist()}\")\n    print(f\"  Words: {[idx_to_word[i] for i in batch_center.tolist()]}\")\n    break  # Only show first batch\n\nprint()\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 5. Skip-Gram Model\n\nNow let's implement the Skip-Gram model in PyTorch!\n\nWe'll implement two versions:\n1. **Basic Skip-Gram**: Uses full softmax (slower, but easier to understand)\n2. **Skip-Gram with Negative Sampling**: Much more efficient for large vocabularies",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 5.1 Skip-Gram Architecture\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                     SKIP-GRAM ARCHITECTURE                       │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                  │\n│  INPUT: center_word_idx (e.g., 5 for \"cat\")                     │\n│         ↓                                                        │\n│  ┌──────────────────────────────────────────┐                   │\n│  │         EMBEDDING LAYER                   │                   │\n│  │   (vocab_size × embedding_dim matrix)    │                   │\n│  │                                          │                   │\n│  │   Row 0: [0.1, 0.2, ..., 0.5]  ← \"the\"  │                   │\n│  │   Row 1: [0.3, 0.1, ..., 0.8]  ← \"cat\"  │                   │\n│  │   Row 2: [0.2, 0.4, ..., 0.3]  ← \"dog\"  │                   │\n│  │   ...                                    │                   │\n│  │                                          │                   │\n│  │   Lookup row 5 → word vector for \"cat\"  │                   │\n│  └──────────────────────────────────────────┘                   │\n│         ↓                                                        │\n│  word_vector: [0.3, 0.1, 0.4, ..., 0.8]  (embedding_dim values) │\n│         ↓                                                        │\n│  ┌──────────────────────────────────────────┐                   │\n│  │         LINEAR LAYER                      │                   │\n│  │   (embedding_dim → vocab_size)           │                   │\n│  │                                          │                   │\n│  │   Computes score for EACH word in vocab  │                   │\n│  └──────────────────────────────────────────┘                   │\n│         ↓                                                        │\n│  scores: [1.2, 3.5, 2.1, ..., 0.8]  (vocab_size values)         │\n│         ↓                                                        │\n│  ┌──────────────────────────────────────────┐                   │\n│  │         SOFTMAX                           │                   │\n│  │   Convert scores to probabilities        │                   │\n│  │   (all values sum to 1.0)                │                   │\n│  └──────────────────────────────────────────┘                   │\n│         ↓                                                        │\n│  OUTPUT: P(context_word | center_word) for all words            │\n│          [0.05, 0.35, 0.15, ..., 0.02]                           │\n│                                                                  │\n└─────────────────────────────────────────────────────────────────┘\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 5.2 Basic Skip-Gram Implementation\n\nLet's implement the basic version with full softmax first.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 5.2 BASIC SKIP-GRAM MODEL\n# ============================================\n\nclass SkipGramBasic(nn.Module):\n    \"\"\"\n    Basic Skip-Gram model using full softmax.\n    \n    This is the simplest implementation:\n    - Input: center word index\n    - Output: probability distribution over all context words\n    \n    Architecture:\n        center_idx -> Embedding -> Linear -> LogSoftmax -> log_probs\n    \"\"\"\n    \n    def __init__(self, vocab_size, embedding_dim):\n        \"\"\"\n        Initialize the Skip-Gram model.\n        \n        Args:\n            vocab_size: Number of unique words in vocabulary\n            embedding_dim: Dimension of word embeddings (e.g., 50, 100, 300)\n        \"\"\"\n        super(SkipGramBasic, self).__init__()\n        \n        # Save parameters\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        \n        # =============================================\n        # LAYER 1: Embedding Layer\n        # =============================================\n        # This is the main thing we're learning!\n        # Maps each word index to a dense vector\n        # Shape: (vocab_size, embedding_dim)\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        \n        # =============================================\n        # LAYER 2: Output Linear Layer\n        # =============================================\n        # Projects embedding to vocabulary size\n        # Each output unit represents score for one word\n        # Shape: (embedding_dim, vocab_size)\n        self.linear = nn.Linear(embedding_dim, vocab_size)\n        \n        # Initialize weights with small random values\n        self._init_weights()\n        \n        print(f\"SkipGramBasic model created:\")\n        print(f\"  Vocab size: {vocab_size}\")\n        print(f\"  Embedding dim: {embedding_dim}\")\n        print(f\"  Embedding layer shape: ({vocab_size}, {embedding_dim})\")\n        print(f\"  Linear layer shape: ({embedding_dim}, {vocab_size})\")\n    \n    def _init_weights(self):\n        \"\"\"Initialize weights with small random values.\"\"\"\n        # Small values help with stable training\n        initrange = 0.5 / self.embedding_dim\n        self.embeddings.weight.data.uniform_(-initrange, initrange)\n        self.linear.weight.data.uniform_(-initrange, initrange)\n        self.linear.bias.data.zero_()\n    \n    def forward(self, center_idx):\n        \"\"\"\n        Forward pass: predict context word probabilities from center word.\n        \n        Args:\n            center_idx: Tensor of shape (batch_size,) with center word indices\n            \n        Returns:\n            log_probs: Tensor of shape (batch_size, vocab_size) with log probabilities\n        \"\"\"\n        # Step 1: Look up embeddings for center words\n        # Input shape: (batch_size,)\n        # Output shape: (batch_size, embedding_dim)\n        embeds = self.embeddings(center_idx)\n        \n        # Step 2: Compute scores for all words in vocabulary\n        # Input shape: (batch_size, embedding_dim)\n        # Output shape: (batch_size, vocab_size)\n        scores = self.linear(embeds)\n        \n        # Step 3: Convert scores to log probabilities\n        # LogSoftmax is more numerically stable than Softmax + Log\n        # Output shape: (batch_size, vocab_size)\n        log_probs = F.log_softmax(scores, dim=1)\n        \n        return log_probs\n    \n    def get_word_embedding(self, word_idx):\n        \"\"\"\n        Get the embedding vector for a specific word.\n        \n        Args:\n            word_idx: Index of the word\n            \n        Returns:\n            Embedding vector of shape (embedding_dim,)\n        \"\"\"\n        return self.embeddings.weight[word_idx].detach().cpu()\n\n# Hyperparameter: embedding dimension\nEMBEDDING_DIM = 50\n\nprint(\"=\" * 60)\nprint(\"CREATING SKIP-GRAM MODEL (BASIC)\")\nprint(\"=\" * 60)\nprint()\n\n# Create the model\nskipgram_basic = SkipGramBasic(vocab_size, EMBEDDING_DIM)\n\nprint()\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# INSPECT THE MODEL\n# ============================================\n\nprint(\"=\" * 60)\nprint(\"MODEL INSPECTION\")\nprint(\"=\" * 60)\nprint()\n\n# Print model architecture\nprint(\"Model Architecture:\")\nprint(skipgram_basic)\nprint()\n\n# Count parameters\ntotal_params = sum(p.numel() for p in skipgram_basic.parameters())\ntrainable_params = sum(p.numel() for p in skipgram_basic.parameters() if p.requires_grad)\n\nprint(\"Parameter Count:\")\nprint(f\"  Total parameters: {total_params:,}\")\nprint(f\"  Trainable parameters: {trainable_params:,}\")\nprint()\n\nprint(\"Parameter Breakdown:\")\nfor name, param in skipgram_basic.named_parameters():\n    print(f\"  {name}: {param.shape} = {param.numel():,} parameters\")\n\nprint()\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5.3 Forward Pass Walkthrough\n\nLet's trace through the forward pass step by step to understand exactly what happens.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 5.3 FORWARD PASS WALKTHROUGH\n# ============================================\n\nprint(\"=\" * 70)\nprint(\"FORWARD PASS WALKTHROUGH\")\nprint(\"=\" * 70)\nprint()\n\n# Let's trace what happens when we input \"cat\"\ncenter_word = \"cat\"\ncenter_idx = word_to_idx[center_word]\ncenter_tensor = torch.tensor([center_idx])  # Shape: (1,) - batch of 1\n\nprint(f\"INPUT:\")\nprint(f\"  Word: '{center_word}'\")\nprint(f\"  Index: {center_idx}\")\nprint(f\"  Tensor shape: {center_tensor.shape}\")\nprint()\n\n# Set model to evaluation mode (disables dropout, etc.)\nskipgram_basic.eval()\n\nwith torch.no_grad():  # Don't compute gradients for this demo\n    \n    # ========== STEP 1: Embedding Lookup ==========\n    embedding = skipgram_basic.embeddings(center_tensor)\n    \n    print(\"STEP 1: EMBEDDING LOOKUP\")\n    print(\"-\" * 50)\n    print(f\"  Input shape: {center_tensor.shape}\")\n    print(f\"  Output shape: {embedding.shape}\")\n    print(f\"  Embedding for '{center_word}':\")\n    print(f\"    First 10 values: {embedding[0, :10].numpy().round(4)}\")\n    print(f\"    Min: {embedding.min().item():.4f}, Max: {embedding.max().item():.4f}\")\n    print()\n    \n    # ========== STEP 2: Linear Transformation ==========\n    scores = skipgram_basic.linear(embedding)\n    \n    print(\"STEP 2: LINEAR TRANSFORMATION (Scores)\")\n    print(\"-\" * 50)\n    print(f\"  Input shape: {embedding.shape}\")\n    print(f\"  Output shape: {scores.shape} (one score per word in vocab)\")\n    print(f\"  Score statistics:\")\n    print(f\"    Min: {scores.min().item():.4f}\")\n    print(f\"    Max: {scores.max().item():.4f}\")\n    print(f\"    Mean: {scores.mean().item():.4f}\")\n    print()\n    \n    # ========== STEP 3: Softmax ==========\n    probs = F.softmax(scores, dim=1)\n    log_probs = F.log_softmax(scores, dim=1)\n    \n    print(\"STEP 3: SOFTMAX (Probabilities)\")\n    print(\"-\" * 50)\n    print(f\"  Input shape: {scores.shape}\")\n    print(f\"  Output shape: {probs.shape}\")\n    print(f\"  Sum of probabilities: {probs.sum().item():.6f} (should be 1.0)\")\n    print()\n    \n    # ========== Show Top Predictions ==========\n    print(\"TOP 10 PREDICTED CONTEXT WORDS (before training):\")\n    print(\"-\" * 50)\n    \n    top_probs, top_indices = probs[0].topk(10)\n    \n    for rank, (prob, idx) in enumerate(zip(top_probs, top_indices), 1):\n        word = idx_to_word[idx.item()]\n        print(f\"  {rank:2d}. '{word}': {prob.item():.4f} ({prob.item()*100:.2f}%)\")\n\nprint()\nprint(\"=\" * 70)\nprint(\"Note: Before training, predictions are essentially random!\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5.4 Skip-Gram with Negative Sampling\n\n**Problem with Basic Skip-Gram**: \n- Computing softmax over the entire vocabulary is expensive!\n- For vocabulary of 100,000 words: 100,000 operations per training step\n\n**Solution - Negative Sampling**:\n- Instead of computing probabilities for ALL words...\n- Only distinguish between the CORRECT context word vs. a few RANDOM \"negative\" words\n- Turns multi-class classification into binary classification\n- Much faster! Only ~5-10 operations per training step\n\n**How it works**:\n- For positive pair (center, context): Model should output HIGH probability\n- For negative pairs (center, random_word): Model should output LOW probability",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 5.4 SKIP-GRAM WITH NEGATIVE SAMPLING\n# ============================================\n\nclass SkipGramNegSampling(nn.Module):\n    \"\"\"\n    Skip-Gram model with Negative Sampling.\n    \n    Key difference from basic Skip-Gram:\n    - Uses TWO embedding matrices (input and output)\n    - Only computes scores for positive and negative samples\n    - Uses binary cross-entropy instead of softmax\n    \n    This is MUCH more efficient for large vocabularies!\n    \"\"\"\n    \n    def __init__(self, vocab_size, embedding_dim):\n        \"\"\"\n        Initialize the model.\n        \n        Args:\n            vocab_size: Number of unique words\n            embedding_dim: Dimension of word embeddings\n        \"\"\"\n        super(SkipGramNegSampling, self).__init__()\n        \n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        \n        # =============================================\n        # INPUT EMBEDDINGS (for center words)\n        # =============================================\n        # These are the embeddings we'll use as our final word vectors\n        self.input_embeddings = nn.Embedding(vocab_size, embedding_dim)\n        \n        # =============================================\n        # OUTPUT EMBEDDINGS (for context/negative words)\n        # =============================================\n        # Separate embeddings used during training\n        # We compute similarity between input and output embeddings\n        self.output_embeddings = nn.Embedding(vocab_size, embedding_dim)\n        \n        # Initialize weights\n        self._init_weights()\n        \n        print(f\"SkipGramNegSampling model created:\")\n        print(f\"  Vocab size: {vocab_size}\")\n        print(f\"  Embedding dim: {embedding_dim}\")\n        print(f\"  Input embeddings: ({vocab_size}, {embedding_dim})\")\n        print(f\"  Output embeddings: ({vocab_size}, {embedding_dim})\")\n    \n    def _init_weights(self):\n        \"\"\"Initialize weights with small random values.\"\"\"\n        initrange = 0.5 / self.embedding_dim\n        self.input_embeddings.weight.data.uniform_(-initrange, initrange)\n        self.output_embeddings.weight.data.uniform_(-initrange, initrange)\n    \n    def forward(self, center_idx, context_idx, negative_indices):\n        \"\"\"\n        Compute negative sampling loss.\n        \n        Args:\n            center_idx: (batch_size,) center word indices\n            context_idx: (batch_size,) positive context word indices\n            negative_indices: (batch_size, num_neg) negative sample indices\n            \n        Returns:\n            loss: Scalar loss value\n        \"\"\"\n        batch_size = center_idx.size(0)\n        num_neg = negative_indices.size(1)\n        \n        # Get embeddings\n        # center: (batch_size, embedding_dim)\n        center_embeds = self.input_embeddings(center_idx)\n        \n        # positive context: (batch_size, embedding_dim)\n        pos_embeds = self.output_embeddings(context_idx)\n        \n        # negative samples: (batch_size, num_neg, embedding_dim)\n        neg_embeds = self.output_embeddings(negative_indices)\n        \n        # =============================================\n        # POSITIVE SCORE\n        # =============================================\n        # Dot product of center and positive context embeddings\n        # Higher = more likely to be a real context word\n        # Shape: (batch_size,)\n        pos_score = torch.sum(center_embeds * pos_embeds, dim=1)\n        \n        # Log-sigmoid of positive score\n        # We want this to be HIGH (close to 0, since log(1) = 0)\n        pos_loss = F.logsigmoid(pos_score)\n        \n        # =============================================\n        # NEGATIVE SCORES\n        # =============================================\n        # Dot product of center with each negative sample\n        # We want these to be LOW\n        # Shape: (batch_size, num_neg)\n        neg_score = torch.bmm(neg_embeds, center_embeds.unsqueeze(2)).squeeze(2)\n        \n        # Log-sigmoid of NEGATIVE score (note the minus sign!)\n        # We want sigmoid(-neg_score) to be HIGH (close to 1)\n        neg_loss = F.logsigmoid(-neg_score).sum(dim=1)\n        \n        # =============================================\n        # TOTAL LOSS\n        # =============================================\n        # Negative because we want to MAXIMIZE log likelihood\n        # But PyTorch optimizers MINIMIZE, so we negate\n        loss = -(pos_loss + neg_loss).mean()\n        \n        return loss\n    \n    def get_word_embedding(self, word_idx):\n        \"\"\"Get the embedding vector for a word (from input embeddings).\"\"\"\n        return self.input_embeddings.weight[word_idx].detach().cpu()\n\nprint(\"=\" * 60)\nprint(\"CREATING SKIP-GRAM MODEL (NEGATIVE SAMPLING)\")\nprint(\"=\" * 60)\nprint()\n\n# Create the model\nskipgram_ns = SkipGramNegSampling(vocab_size, EMBEDDING_DIM)\n\nprint()\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5.5 Negative Sampling Function\n\nWe need a function to sample random \"negative\" words. These are words that are NOT the actual context word.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 5.5 NEGATIVE SAMPLING FUNCTION\n# ============================================\n\ndef get_negative_samples(batch_size, num_neg, vocab_size, positive_indices):\n    \"\"\"\n    Sample negative words for negative sampling.\n    \n    For each positive sample, we randomly select num_neg words from the vocabulary\n    that are NOT the positive context word.\n    \n    Args:\n        batch_size: Number of samples in the batch\n        num_neg: Number of negative samples per positive sample\n        vocab_size: Size of vocabulary\n        positive_indices: Tensor of positive context word indices to avoid\n        \n    Returns:\n        Tensor of shape (batch_size, num_neg) with negative sample indices\n    \"\"\"\n    negative_samples = []\n    \n    for i in range(batch_size):\n        neg = []\n        positive = positive_indices[i].item()\n        \n        while len(neg) < num_neg:\n            # Random word from vocabulary\n            sample = random.randint(0, vocab_size - 1)\n            \n            # Make sure it's not the positive word\n            if sample != positive:\n                neg.append(sample)\n        \n        negative_samples.append(neg)\n    \n    return torch.tensor(negative_samples, dtype=torch.long)\n\n# Test the negative sampling function\nNUM_NEGATIVE = 5  # Number of negative samples\n\nprint(\"=\" * 70)\nprint(\"NEGATIVE SAMPLING EXAMPLE\")\nprint(\"=\" * 70)\nprint()\n\n# Example: positive words are \"cat\" and \"dog\"\ntest_positive = torch.tensor([word_to_idx[\"cat\"], word_to_idx[\"dog\"]])\ntest_negative = get_negative_samples(2, NUM_NEGATIVE, vocab_size, test_positive)\n\nprint(f\"Number of negative samples per positive: {NUM_NEGATIVE}\")\nprint()\n\nfor i in range(2):\n    pos_word = idx_to_word[test_positive[i].item()]\n    neg_words = [idx_to_word[idx] for idx in test_negative[i].tolist()]\n    \n    print(f\"Sample {i+1}:\")\n    print(f\"  Positive word: '{pos_word}' (index {test_positive[i].item()})\")\n    print(f\"  Negative indices: {test_negative[i].tolist()}\")\n    print(f\"  Negative words: {neg_words}\")\n    print()\n\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 6. CBOW (Continuous Bag of Words) Model\n\nCBOW is the \"opposite\" of Skip-Gram:\n- **Skip-Gram**: center word → predict context words\n- **CBOW**: context words → predict center word\n\nThe key difference is that CBOW **averages** all context word embeddings before making a prediction.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 6.1 CBOW Architecture\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                       CBOW ARCHITECTURE                          │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                  │\n│  INPUT: context_word_indices (e.g., [0, 1, 3, 0] for            │\n│         [\"the\", \"cat\", \"on\", \"the\"])                            │\n│         ↓                                                        │\n│  ┌──────────────────────────────────────────┐                   │\n│  │         EMBEDDING LAYER                   │                   │\n│  │   Look up embedding for EACH context word │                   │\n│  │                                          │                   │\n│  │   \"the\" → [0.1, 0.2, ..., 0.5]          │                   │\n│  │   \"cat\" → [0.3, 0.1, ..., 0.8]          │                   │\n│  │   \"on\"  → [0.4, 0.3, ..., 0.2]          │                   │\n│  │   \"the\" → [0.1, 0.2, ..., 0.5]          │                   │\n│  └──────────────────────────────────────────┘                   │\n│         ↓                                                        │\n│  4 vectors, each of shape (embedding_dim,)                      │\n│         ↓                                                        │\n│  ┌──────────────────────────────────────────┐                   │\n│  │         AVERAGE                           │                   │\n│  │   Average all context embeddings         │                   │\n│  │   (0.1+0.3+0.4+0.1)/4 = 0.225, ...      │                   │\n│  └──────────────────────────────────────────┘                   │\n│         ↓                                                        │\n│  avg_vector: [0.225, 0.2, ..., 0.5]  (one vector!)              │\n│         ↓                                                        │\n│  ┌──────────────────────────────────────────┐                   │\n│  │         LINEAR + SOFTMAX                  │                   │\n│  │   Same as Skip-Gram output layer         │                   │\n│  └──────────────────────────────────────────┘                   │\n│         ↓                                                        │\n│  OUTPUT: P(center_word | context_words)                         │\n│          Probability for each word in vocabulary                 │\n│                                                                  │\n└─────────────────────────────────────────────────────────────────┘\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 6.2 CBOW Implementation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 6.2 CBOW MODEL IMPLEMENTATION\n# ============================================\n\nclass CBOW(nn.Module):\n    \"\"\"\n    Continuous Bag of Words (CBOW) model.\n    \n    Predicts the center word given surrounding context words.\n    \n    Key steps:\n    1. Look up embeddings for ALL context words\n    2. AVERAGE the context embeddings into one vector\n    3. Use that vector to predict the center word\n    \n    Architecture:\n        context_indices -> Embeddings -> Average -> Linear -> LogSoftmax -> log_probs\n    \"\"\"\n    \n    def __init__(self, vocab_size, embedding_dim):\n        \"\"\"\n        Initialize the CBOW model.\n        \n        Args:\n            vocab_size: Number of unique words\n            embedding_dim: Dimension of word embeddings\n        \"\"\"\n        super(CBOW, self).__init__()\n        \n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        \n        # Embedding layer (same as Skip-Gram)\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        \n        # Output linear layer (same as Skip-Gram)\n        self.linear = nn.Linear(embedding_dim, vocab_size)\n        \n        # Initialize weights\n        self._init_weights()\n        \n        print(f\"CBOW model created:\")\n        print(f\"  Vocab size: {vocab_size}\")\n        print(f\"  Embedding dim: {embedding_dim}\")\n    \n    def _init_weights(self):\n        \"\"\"Initialize weights with small random values.\"\"\"\n        initrange = 0.5 / self.embedding_dim\n        self.embeddings.weight.data.uniform_(-initrange, initrange)\n        self.linear.weight.data.uniform_(-initrange, initrange)\n        self.linear.bias.data.zero_()\n    \n    def forward(self, context_indices, context_lengths):\n        \"\"\"\n        Forward pass: predict center word from context words.\n        \n        Args:\n            context_indices: (batch_size, max_context_size) context word indices\n                            Padded with -1 for variable length contexts\n            context_lengths: (batch_size,) actual number of context words per sample\n            \n        Returns:\n            log_probs: (batch_size, vocab_size) log probabilities\n        \"\"\"\n        batch_size = context_indices.size(0)\n        \n        # =============================================\n        # STEP 1: Handle padding (-1 values)\n        # =============================================\n        # Replace -1 (padding) with 0 for embedding lookup\n        # We'll zero out these positions later\n        context_safe = context_indices.clamp(min=0)\n        \n        # =============================================\n        # STEP 2: Get embeddings for all context words\n        # =============================================\n        # Shape: (batch_size, max_context_size, embedding_dim)\n        embeds = self.embeddings(context_safe)\n        \n        # =============================================\n        # STEP 3: Create mask for padding\n        # =============================================\n        # True where NOT padded, False where padded\n        # Shape: (batch_size, max_context_size, 1)\n        mask = (context_indices >= 0).float().unsqueeze(2)\n        \n        # =============================================\n        # STEP 4: Apply mask and compute average\n        # =============================================\n        # Zero out padded positions\n        masked_embeds = embeds * mask\n        \n        # Sum of context embeddings\n        # Shape: (batch_size, embedding_dim)\n        sum_embeds = masked_embeds.sum(dim=1)\n        \n        # Divide by actual context length (not max_context_size!)\n        # Shape: (batch_size, 1)\n        lengths = context_lengths.float().unsqueeze(1)\n        avg_embeds = sum_embeds / lengths\n        \n        # =============================================\n        # STEP 5: Linear layer and softmax\n        # =============================================\n        scores = self.linear(avg_embeds)\n        log_probs = F.log_softmax(scores, dim=1)\n        \n        return log_probs\n    \n    def get_word_embedding(self, word_idx):\n        \"\"\"Get the embedding vector for a word.\"\"\"\n        return self.embeddings.weight[word_idx].detach().cpu()\n\nprint(\"=\" * 60)\nprint(\"CREATING CBOW MODEL\")\nprint(\"=\" * 60)\nprint()\n\n# Create the model\ncbow_model = CBOW(vocab_size, EMBEDDING_DIM)\n\nprint()\nprint(\"Model Architecture:\")\nprint(cbow_model)\nprint()\n\n# Count parameters\ntotal_params = sum(p.numel() for p in cbow_model.parameters())\nprint(f\"Total parameters: {total_params:,}\")\nprint()\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 6.3 CBOW Forward Pass Walkthrough\n\nLet's trace through the CBOW forward pass step by step.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 6.3 CBOW FORWARD PASS WALKTHROUGH\n# ============================================\n\nprint(\"=\" * 70)\nprint(\"CBOW FORWARD PASS WALKTHROUGH\")\nprint(\"=\" * 70)\nprint()\n\n# Example: predicting \"sat\" from context [\"the\", \"cat\", \"on\", \"the\"]\ncontext_words = [\"the\", \"cat\", \"on\", \"the\"]\ncenter_word = \"sat\"\n\ncontext_indices = [word_to_idx[w] for w in context_words]\ncontext_tensor = torch.tensor([context_indices])  # Shape: (1, 4)\nlength_tensor = torch.tensor([len(context_indices)])\n\nprint(f\"INPUT:\")\nprint(f\"  Context words: {context_words}\")\nprint(f\"  Context indices: {context_indices}\")\nprint(f\"  Context tensor shape: {context_tensor.shape}\")\nprint(f\"  Target (center word): '{center_word}'\")\nprint()\n\ncbow_model.eval()\n\nwith torch.no_grad():\n    \n    # ========== STEP 1: Get embeddings for each context word ==========\n    embeds = cbow_model.embeddings(context_tensor)\n    \n    print(\"STEP 1: EMBEDDING LOOKUP FOR EACH CONTEXT WORD\")\n    print(\"-\" * 60)\n    print(f\"  Output shape: {embeds.shape} (batch, num_context, embed_dim)\")\n    print()\n    \n    for i, word in enumerate(context_words):\n        print(f\"  '{word}' embedding (first 8 values): {embeds[0, i, :8].numpy().round(4)}\")\n    print()\n    \n    # ========== STEP 2: Average the embeddings ==========\n    avg_embed = embeds.mean(dim=1)\n    \n    print(\"STEP 2: AVERAGE CONTEXT EMBEDDINGS\")\n    print(\"-\" * 60)\n    print(f\"  Output shape: {avg_embed.shape} (batch, embed_dim)\")\n    print(f\"  Averaged embedding (first 8 values): {avg_embed[0, :8].numpy().round(4)}\")\n    print()\n    \n    # ========== STEP 3: Linear transformation ==========\n    scores = cbow_model.linear(avg_embed)\n    \n    print(\"STEP 3: LINEAR TRANSFORMATION\")\n    print(\"-\" * 60)\n    print(f\"  Output shape: {scores.shape}\")\n    print(f\"  Score statistics: min={scores.min():.4f}, max={scores.max():.4f}\")\n    print()\n    \n    # ========== STEP 4: Softmax ==========\n    probs = F.softmax(scores, dim=1)\n    \n    print(\"STEP 4: SOFTMAX (Probabilities)\")\n    print(\"-\" * 60)\n    print(f\"  Sum of probabilities: {probs.sum().item():.6f}\")\n    print()\n    \n    # ========== Show top predictions ==========\n    print(\"TOP 10 PREDICTIONS (before training):\")\n    print(\"-\" * 60)\n    \n    top_probs, top_indices = probs[0].topk(10)\n    \n    for rank, (prob, idx) in enumerate(zip(top_probs, top_indices), 1):\n        word = idx_to_word[idx.item()]\n        marker = \" <-- TARGET\" if word == center_word else \"\"\n        print(f\"  {rank:2d}. '{word}': {prob.item():.4f} ({prob.item()*100:.2f}%){marker}\")\n\nprint()\nprint(\"=\" * 70)\nprint(\"Note: Before training, the target word 'sat' is likely not in top predictions!\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 7. Training the Models\n\nNow let's train our models! The training loop follows this pattern:\n\n```\nfor each epoch:\n    for each batch:\n        1. Forward pass: compute predictions\n        2. Compute loss: how wrong were we?\n        3. Backward pass: compute gradients\n        4. Update weights: improve the model\n```\n\n### Key Components:\n- **Loss Function**: Measures prediction error\n- **Optimizer**: Updates weights to minimize loss\n- **Learning Rate**: How big the weight updates are",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 7.1 TRAINING HYPERPARAMETERS\n# ============================================\n\n# Training settings\nLEARNING_RATE = 0.01      # How fast the model learns\nNUM_EPOCHS = 100          # How many times to go through the data\nNUM_NEGATIVE = 5          # Number of negative samples (for neg sampling)\n\nprint(\"=\" * 60)\nprint(\"TRAINING HYPERPARAMETERS\")\nprint(\"=\" * 60)\nprint()\nprint(f\"Embedding dimension:    {EMBEDDING_DIM}\")\nprint(f\"Batch size:             {BATCH_SIZE}\")\nprint(f\"Learning rate:          {LEARNING_RATE}\")\nprint(f\"Number of epochs:       {NUM_EPOCHS}\")\nprint(f\"Negative samples:       {NUM_NEGATIVE}\")\nprint()\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.2 Training Skip-Gram (Basic)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 7.2 TRAIN SKIP-GRAM (BASIC)\n# ============================================\n\ndef train_skipgram_basic(model, dataloader, num_epochs, learning_rate, device):\n    \"\"\"\n    Train the basic Skip-Gram model.\n    \n    Args:\n        model: SkipGramBasic model\n        dataloader: DataLoader with training data\n        num_epochs: Number of training epochs\n        learning_rate: Learning rate for optimizer\n        device: Device to train on (CPU/GPU)\n        \n    Returns:\n        losses: List of average loss per epoch\n    \"\"\"\n    # Move model to device\n    model = model.to(device)\n    model.train()  # Set to training mode\n    \n    # Loss function: Negative Log-Likelihood\n    # This is the standard loss for classification with log_softmax output\n    criterion = nn.NLLLoss()\n    \n    # Optimizer: Adam (adaptive learning rate)\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    \n    # Track losses\n    losses = []\n    \n    print(\"=\" * 60)\n    print(\"TRAINING SKIP-GRAM (BASIC)\")\n    print(\"=\" * 60)\n    print(f\"Device: {device}\")\n    print(f\"Epochs: {num_epochs}\")\n    print(f\"Learning rate: {learning_rate}\")\n    print(\"-\" * 60)\n    \n    for epoch in range(num_epochs):\n        total_loss = 0\n        num_batches = 0\n        \n        for center, context in dataloader:\n            # Move data to device\n            center = center.to(device)\n            context = context.to(device)\n            \n            # ========== STEP 1: Forward pass ==========\n            # Get predictions (log probabilities)\n            log_probs = model(center)\n            \n            # ========== STEP 2: Compute loss ==========\n            # Compare predictions to actual context words\n            loss = criterion(log_probs, context)\n            \n            # ========== STEP 3: Backward pass ==========\n            # Clear previous gradients\n            optimizer.zero_grad()\n            # Compute gradients\n            loss.backward()\n            \n            # ========== STEP 4: Update weights ==========\n            optimizer.step()\n            \n            total_loss += loss.item()\n            num_batches += 1\n        \n        # Calculate average loss for this epoch\n        avg_loss = total_loss / num_batches\n        losses.append(avg_loss)\n        \n        # Print progress every 10 epochs\n        if (epoch + 1) % 10 == 0 or epoch == 0:\n            print(f\"Epoch {epoch+1:3d}/{num_epochs} | Loss: {avg_loss:.4f}\")\n    \n    print(\"-\" * 60)\n    print(f\"Training complete! Final loss: {losses[-1]:.4f}\")\n    print(\"=\" * 60)\n    \n    return losses\n\n# Re-create the model (fresh weights)\nskipgram_basic = SkipGramBasic(vocab_size, EMBEDDING_DIM)\n\n# Train the model\nskipgram_basic_losses = train_skipgram_basic(\n    skipgram_basic, \n    skipgram_loader, \n    NUM_EPOCHS, \n    LEARNING_RATE, \n    device\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.3 Training Skip-Gram (Negative Sampling)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 7.3 TRAIN SKIP-GRAM (NEGATIVE SAMPLING)\n# ============================================\n\ndef train_skipgram_negative_sampling(model, pairs, num_epochs, learning_rate, \n                                     batch_size, num_neg, vocab_size, device):\n    \"\"\"\n    Train Skip-Gram model with Negative Sampling.\n    \n    Note: We use the raw pairs instead of DataLoader because we need\n    to generate negative samples dynamically.\n    \"\"\"\n    model = model.to(device)\n    model.train()\n    \n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    \n    losses = []\n    \n    print(\"=\" * 60)\n    print(\"TRAINING SKIP-GRAM (NEGATIVE SAMPLING)\")\n    print(\"=\" * 60)\n    print(f\"Device: {device}\")\n    print(f\"Epochs: {num_epochs}\")\n    print(f\"Negative samples: {num_neg}\")\n    print(\"-\" * 60)\n    \n    for epoch in range(num_epochs):\n        # Shuffle pairs at the start of each epoch\n        random.shuffle(pairs)\n        \n        total_loss = 0\n        num_batches = 0\n        \n        # Process in batches\n        for i in range(0, len(pairs), batch_size):\n            batch = pairs[i:i+batch_size]\n            \n            # Extract center and context indices\n            center_indices = torch.tensor([p[0] for p in batch], dtype=torch.long).to(device)\n            context_indices = torch.tensor([p[1] for p in batch], dtype=torch.long).to(device)\n            \n            # Generate negative samples\n            negative_indices = get_negative_samples(\n                len(batch), num_neg, vocab_size, context_indices\n            ).to(device)\n            \n            # Forward pass (returns loss directly)\n            loss = model(center_indices, context_indices, negative_indices)\n            \n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            num_batches += 1\n        \n        avg_loss = total_loss / num_batches\n        losses.append(avg_loss)\n        \n        if (epoch + 1) % 10 == 0 or epoch == 0:\n            print(f\"Epoch {epoch+1:3d}/{num_epochs} | Loss: {avg_loss:.4f}\")\n    \n    print(\"-\" * 60)\n    print(f\"Training complete! Final loss: {losses[-1]:.4f}\")\n    print(\"=\" * 60)\n    \n    return losses\n\n# Re-create the model (fresh weights)\nskipgram_ns = SkipGramNegSampling(vocab_size, EMBEDDING_DIM)\n\n# Train the model\nskipgram_ns_losses = train_skipgram_negative_sampling(\n    skipgram_ns,\n    skipgram_pairs,\n    NUM_EPOCHS,\n    LEARNING_RATE,\n    BATCH_SIZE,\n    NUM_NEGATIVE,\n    vocab_size,\n    device\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.4 Training CBOW",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 7.4 TRAIN CBOW\n# ============================================\n\ndef train_cbow(model, dataloader, num_epochs, learning_rate, device):\n    \"\"\"\n    Train the CBOW model.\n    \"\"\"\n    model = model.to(device)\n    model.train()\n    \n    criterion = nn.NLLLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    \n    losses = []\n    \n    print(\"=\" * 60)\n    print(\"TRAINING CBOW\")\n    print(\"=\" * 60)\n    print(f\"Device: {device}\")\n    print(f\"Epochs: {num_epochs}\")\n    print(f\"Learning rate: {learning_rate}\")\n    print(\"-\" * 60)\n    \n    for epoch in range(num_epochs):\n        total_loss = 0\n        num_batches = 0\n        \n        for context, center, lengths in dataloader:\n            context = context.to(device)\n            center = center.to(device)\n            lengths = lengths.to(device)\n            \n            # Forward pass\n            log_probs = model(context, lengths)\n            \n            # Compute loss\n            loss = criterion(log_probs, center)\n            \n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            num_batches += 1\n        \n        avg_loss = total_loss / num_batches\n        losses.append(avg_loss)\n        \n        if (epoch + 1) % 10 == 0 or epoch == 0:\n            print(f\"Epoch {epoch+1:3d}/{num_epochs} | Loss: {avg_loss:.4f}\")\n    \n    print(\"-\" * 60)\n    print(f\"Training complete! Final loss: {losses[-1]:.4f}\")\n    print(\"=\" * 60)\n    \n    return losses\n\n# Re-create the model (fresh weights)\ncbow_model = CBOW(vocab_size, EMBEDDING_DIM)\n\n# Train the model\ncbow_losses = train_cbow(\n    cbow_model,\n    cbow_loader,\n    NUM_EPOCHS,\n    LEARNING_RATE,\n    device\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7.5 Visualize Training Loss",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 7.5 VISUALIZE TRAINING LOSS\n# ============================================\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Plot 1: Skip-Gram Basic\nax1 = axes[0]\nax1.plot(skipgram_basic_losses, color='blue', linewidth=2)\nax1.set_xlabel('Epoch', fontsize=12)\nax1.set_ylabel('Loss', fontsize=12)\nax1.set_title('Skip-Gram (Basic)\\nTraining Loss', fontsize=14, fontweight='bold')\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Skip-Gram Negative Sampling\nax2 = axes[1]\nax2.plot(skipgram_ns_losses, color='green', linewidth=2)\nax2.set_xlabel('Epoch', fontsize=12)\nax2.set_ylabel('Loss', fontsize=12)\nax2.set_title('Skip-Gram (Neg Sampling)\\nTraining Loss', fontsize=14, fontweight='bold')\nax2.grid(True, alpha=0.3)\n\n# Plot 3: CBOW\nax3 = axes[2]\nax3.plot(cbow_losses, color='red', linewidth=2)\nax3.set_xlabel('Epoch', fontsize=12)\nax3.set_ylabel('Loss', fontsize=12)\nax3.set_title('CBOW\\nTraining Loss', fontsize=14, fontweight='bold')\nax3.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Summary table\nprint(\"=\" * 60)\nprint(\"TRAINING SUMMARY\")\nprint(\"=\" * 60)\nprint()\nprint(f\"{'Model':<30} {'Initial Loss':<15} {'Final Loss':<15}\")\nprint(\"-\" * 60)\nprint(f\"{'Skip-Gram (Basic)':<30} {skipgram_basic_losses[0]:<15.4f} {skipgram_basic_losses[-1]:<15.4f}\")\nprint(f\"{'Skip-Gram (Neg Sampling)':<30} {skipgram_ns_losses[0]:<15.4f} {skipgram_ns_losses[-1]:<15.4f}\")\nprint(f\"{'CBOW':<30} {cbow_losses[0]:<15.4f} {cbow_losses[-1]:<15.4f}\")\nprint(\"-\" * 60)\nprint()\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 8. Evaluation & Visualization\n\nNow that our models are trained, let's evaluate the learned embeddings!\n\nWe'll look at:\n1. **Word Similarity**: Using cosine similarity\n2. **t-SNE Visualization**: 2D visualization of embeddings\n3. **Model Comparison**: Compare Skip-Gram vs CBOW",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 8.1 Word Similarity with Cosine Similarity\n\n**Cosine similarity** measures the angle between two vectors:\n\n$$\\text{cosine}(\\vec{u}, \\vec{v}) = \\frac{\\vec{u} \\cdot \\vec{v}}{||\\vec{u}|| \\times ||\\vec{v}||}$$\n\n- **1.0**: Identical direction (most similar)\n- **0.0**: Perpendicular (no similarity)\n- **-1.0**: Opposite direction (least similar)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 8.1 WORD SIMILARITY FUNCTIONS\n# ============================================\n\ndef cosine_similarity(vec1, vec2):\n    \"\"\"\n    Compute cosine similarity between two vectors.\n    \n    Args:\n        vec1, vec2: Tensors of same shape\n        \n    Returns:\n        Cosine similarity (float between -1 and 1)\n    \"\"\"\n    dot_product = torch.dot(vec1, vec2)\n    norm1 = torch.norm(vec1)\n    norm2 = torch.norm(vec2)\n    return (dot_product / (norm1 * norm2)).item()\n\ndef find_similar_words(model, word, word_to_idx, idx_to_word, top_k=5):\n    \"\"\"\n    Find the most similar words to a given word.\n    \n    Args:\n        model: Trained Word2Vec model\n        word: Word to find similar words for\n        word_to_idx: Word to index mapping\n        idx_to_word: Index to word mapping\n        top_k: Number of similar words to return\n        \n    Returns:\n        List of (word, similarity) tuples\n    \"\"\"\n    if word not in word_to_idx:\n        print(f\"'{word}' not in vocabulary!\")\n        return []\n    \n    word_idx = word_to_idx[word]\n    word_vec = model.get_word_embedding(word_idx)\n    \n    similarities = []\n    \n    for idx in range(len(idx_to_word)):\n        if idx != word_idx:  # Skip the word itself\n            other_vec = model.get_word_embedding(idx)\n            sim = cosine_similarity(word_vec, other_vec)\n            similarities.append((idx_to_word[idx], sim))\n    \n    # Sort by similarity (descending)\n    similarities.sort(key=lambda x: -x[1])\n    \n    return similarities[:top_k]\n\n# Test the similarity function\nprint(\"=\" * 60)\nprint(\"TESTING COSINE SIMILARITY\")\nprint(\"=\" * 60)\nprint()\n\n# Get embeddings for cat and dog\ncat_vec = skipgram_ns.get_word_embedding(word_to_idx[\"cat\"])\ndog_vec = skipgram_ns.get_word_embedding(word_to_idx[\"dog\"])\nthe_vec = skipgram_ns.get_word_embedding(word_to_idx[\"the\"])\n\nprint(f\"Cosine similarity (cat, dog): {cosine_similarity(cat_vec, dog_vec):.4f}\")\nprint(f\"Cosine similarity (cat, the): {cosine_similarity(cat_vec, the_vec):.4f}\")\nprint(f\"Cosine similarity (cat, cat): {cosine_similarity(cat_vec, cat_vec):.4f}\")\nprint()\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# FIND SIMILAR WORDS FOR TEST WORDS\n# ============================================\n\ntest_words = [\"cat\", \"dog\", \"the\", \"sat\", \"love\"]\n\nprint(\"=\" * 70)\nprint(\"MOST SIMILAR WORDS (Skip-Gram with Negative Sampling)\")\nprint(\"=\" * 70)\nprint()\n\nfor word in test_words:\n    if word in word_to_idx:\n        print(f\"Words most similar to '{word}':\")\n        similar = find_similar_words(skipgram_ns, word, word_to_idx, idx_to_word, top_k=5)\n        for rank, (sim_word, score) in enumerate(similar, 1):\n            print(f\"  {rank}. '{sim_word}': {score:.4f}\")\n        print()\n\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 8.2 t-SNE Visualization\n\n**t-SNE** (t-Distributed Stochastic Neighbor Embedding) reduces high-dimensional embeddings to 2D for visualization.\n\nWords that are close in the embedding space should be close in the 2D visualization.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 8.2 t-SNE VISUALIZATION\n# ============================================\n\ndef visualize_embeddings_tsne(model, idx_to_word, title=\"Word Embeddings\"):\n    \"\"\"\n    Visualize word embeddings using t-SNE.\n    \n    Args:\n        model: Trained Word2Vec model\n        idx_to_word: Index to word mapping\n        title: Plot title\n    \"\"\"\n    # Collect all embeddings\n    embeddings = []\n    words = []\n    \n    for idx in range(len(idx_to_word)):\n        embedding = model.get_word_embedding(idx).numpy()\n        embeddings.append(embedding)\n        words.append(idx_to_word[idx])\n    \n    embeddings = np.array(embeddings)\n    \n    print(f\"Embeddings shape: {embeddings.shape}\")\n    \n    # Apply t-SNE\n    # perplexity should be less than number of samples\n    perplexity = min(5, len(words) - 1)\n    \n    tsne = TSNE(n_components=2, random_state=SEED, perplexity=perplexity, n_iter=1000)\n    embeddings_2d = tsne.fit_transform(embeddings)\n    \n    # Create plot\n    plt.figure(figsize=(12, 10))\n    \n    # Scatter plot\n    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n                c='steelblue', s=100, alpha=0.7, edgecolors='navy')\n    \n    # Add labels for each point\n    for i, word in enumerate(words):\n        plt.annotate(word, \n                    xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]),\n                    xytext=(5, 5), textcoords='offset points',\n                    fontsize=11, fontweight='bold',\n                    color='darkred')\n    \n    plt.title(title, fontsize=16, fontweight='bold')\n    plt.xlabel('t-SNE Dimension 1', fontsize=12)\n    plt.ylabel('t-SNE Dimension 2', fontsize=12)\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n\n# Visualize embeddings from Skip-Gram (Negative Sampling)\nprint(\"=\" * 60)\nprint(\"t-SNE VISUALIZATION\")\nprint(\"=\" * 60)\nprint()\n\nvisualize_embeddings_tsne(skipgram_ns, idx_to_word, \n                          \"Skip-Gram (Neg Sampling) Word Embeddings\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize CBOW embeddings for comparison\nvisualize_embeddings_tsne(cbow_model, idx_to_word, \n                          \"CBOW Word Embeddings\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 8.3 Model Comparison\n\nLet's compare the similarity results from different models side by side.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 8.3 MODEL COMPARISON\n# ============================================\n\ndef compare_models(models_dict, test_words, word_to_idx, idx_to_word, top_k=3):\n    \"\"\"\n    Compare similar words across multiple models.\n    \"\"\"\n    print(\"=\" * 90)\n    print(\"MODEL COMPARISON: Most Similar Words\")\n    print(\"=\" * 90)\n    print()\n    \n    for word in test_words:\n        if word not in word_to_idx:\n            continue\n            \n        print(f\"Similar to '{word}':\")\n        print(\"-\" * 90)\n        \n        # Header\n        header = f\"{'Rank':<6}\"\n        for model_name in models_dict.keys():\n            header += f\"{model_name:<28}\"\n        print(header)\n        print(\"-\" * 90)\n        \n        # Get similar words from each model\n        all_similar = {}\n        for model_name, model in models_dict.items():\n            all_similar[model_name] = find_similar_words(\n                model, word, word_to_idx, idx_to_word, top_k\n            )\n        \n        # Print row by row\n        for i in range(top_k):\n            row = f\"{i+1:<6}\"\n            for model_name in models_dict.keys():\n                if i < len(all_similar[model_name]):\n                    w, s = all_similar[model_name][i]\n                    row += f\"'{w}' ({s:.3f}){'':12}\"\n                else:\n                    row += f\"{'N/A':<28}\"\n            print(row)\n        \n        print()\n    \n    print(\"=\" * 90)\n\n# Create dictionary of models\nmodels = {\n    \"Skip-Gram (Basic)\": skipgram_basic,\n    \"Skip-Gram (NegSamp)\": skipgram_ns,\n    \"CBOW\": cbow_model\n}\n\n# Compare models\ncompare_models(models, [\"cat\", \"dog\", \"the\"], word_to_idx, idx_to_word, top_k=5)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 9. Playing with Results\n\nLet's have some fun with our learned embeddings!\n\n### 9.1 Word Arithmetic (Vector Operations)\n\nOne of the most famous properties of Word2Vec is that word embeddings can capture **semantic relationships** through vector arithmetic.\n\nThe classic example:\n```\nking - man + woman ≈ queen\n```\n\nThis works because the vector `king - man` captures the concept of \"royalty without maleness\", and adding `woman` gives us \"royalty with femaleness\" = queen.\n\nNote: Our vocabulary is small, so results may not be perfect, but let's try!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 9.1 WORD ARITHMETIC\n# ============================================\n\ndef word_arithmetic(model, positive_words, negative_words, word_to_idx, idx_to_word, top_k=5):\n    \"\"\"\n    Perform word vector arithmetic: sum of positive words - sum of negative words.\n    \n    Example: word_arithmetic([\"king\", \"woman\"], [\"man\"]) should return \"queen\"\n    \n    Args:\n        model: Trained Word2Vec model\n        positive_words: List of words to add\n        negative_words: List of words to subtract\n        word_to_idx: Word to index mapping\n        idx_to_word: Index to word mapping\n        top_k: Number of results to return\n        \n    Returns:\n        List of (word, similarity) tuples\n    \"\"\"\n    # Start with zero vector\n    result_vec = torch.zeros(model.embedding_dim)\n    \n    # Build equation string for display\n    equation = \" + \".join(positive_words)\n    if negative_words:\n        equation += \" - \" + \" - \".join(negative_words)\n    \n    print(f\"Computing: {equation}\")\n    print(\"-\" * 50)\n    \n    # Add positive word vectors\n    for word in positive_words:\n        if word in word_to_idx:\n            vec = model.get_word_embedding(word_to_idx[word])\n            result_vec += vec\n            print(f\"  + '{word}'\")\n        else:\n            print(f\"  Warning: '{word}' not in vocabulary!\")\n    \n    # Subtract negative word vectors\n    for word in negative_words:\n        if word in word_to_idx:\n            vec = model.get_word_embedding(word_to_idx[word])\n            result_vec -= vec\n            print(f\"  - '{word}'\")\n        else:\n            print(f\"  Warning: '{word}' not in vocabulary!\")\n    \n    print(\"-\" * 50)\n    \n    # Find words most similar to the result vector\n    # Exclude input words from results\n    exclude = set(positive_words + negative_words)\n    \n    similarities = []\n    for idx in range(len(idx_to_word)):\n        word = idx_to_word[idx]\n        if word not in exclude:\n            word_vec = model.get_word_embedding(idx)\n            sim = cosine_similarity(result_vec, word_vec)\n            similarities.append((word, sim))\n    \n    # Sort by similarity\n    similarities.sort(key=lambda x: -x[1])\n    \n    print(f\"Result (top {top_k} matches):\")\n    for rank, (word, sim) in enumerate(similarities[:top_k], 1):\n        print(f\"  {rank}. '{word}': {sim:.4f}\")\n    \n    return similarities[:top_k]\n\nprint(\"=\" * 60)\nprint(\"WORD ARITHMETIC EXAMPLES\")\nprint(\"=\" * 60)\nprint()\n\n# Try some word arithmetic with our vocabulary\n# Since we have a pet-focused corpus, let's try relevant examples\n\nprint(\"Example 1: cat + dog (combining pet concepts)\")\nprint()\nword_arithmetic(skipgram_ns, [\"cat\", \"dog\"], [], word_to_idx, idx_to_word)\n\nprint()\nprint()\n\nprint(\"Example 2: cat - the (removing common word influence)\")\nprint()\nword_arithmetic(skipgram_ns, [\"cat\"], [\"the\"], word_to_idx, idx_to_word)\n\nprint()\nprint()\n\nprint(\"Example 3: sat + sleeping (action combination)\")\nprint()\nword_arithmetic(skipgram_ns, [\"sat\", \"sleeping\"], [], word_to_idx, idx_to_word)\n\nprint()\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 9.2 Interactive Word Explorer\n\nLet's create a function to explore a word's embedding across all models.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 9.2 INTERACTIVE WORD EXPLORER\n# ============================================\n\ndef explore_word(word, models_dict, word_to_idx, idx_to_word):\n    \"\"\"\n    Explore a word's embedding across multiple models.\n    \n    Args:\n        word: Word to explore\n        models_dict: Dictionary of {model_name: model}\n        word_to_idx: Word to index mapping\n        idx_to_word: Index to word mapping\n    \"\"\"\n    print(\"=\" * 70)\n    print(f\"EXPLORING WORD: '{word}'\")\n    print(\"=\" * 70)\n    print()\n    \n    if word not in word_to_idx:\n        print(f\"'{word}' is not in the vocabulary!\")\n        return\n    \n    word_idx = word_to_idx[word]\n    print(f\"Word index: {word_idx}\")\n    print(f\"Word frequency in corpus: {word_counts[word]}\")\n    print()\n    \n    for model_name, model in models_dict.items():\n        print(f\"--- {model_name} ---\")\n        \n        # Get embedding\n        embedding = model.get_word_embedding(word_idx)\n        \n        # Embedding statistics\n        print(f\"  Embedding shape: {embedding.shape}\")\n        print(f\"  Embedding norm: {torch.norm(embedding).item():.4f}\")\n        print(f\"  Min value: {embedding.min().item():.4f}\")\n        print(f\"  Max value: {embedding.max().item():.4f}\")\n        print(f\"  Mean value: {embedding.mean().item():.4f}\")\n        print(f\"  First 10 values: {embedding[:10].numpy().round(4)}\")\n        \n        # Similar words\n        similar = find_similar_words(model, word, word_to_idx, idx_to_word, top_k=5)\n        print(f\"  Most similar: {[(w, f'{s:.3f}') for w, s in similar]}\")\n        print()\n    \n    print(\"=\" * 70)\n\n# Explore some words\nfor word in [\"cat\", \"dog\"]:\n    explore_word(word, models, word_to_idx, idx_to_word)\n    print()\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 9.3 Similarity Heatmap\n\nLet's visualize the similarity between all words as a heatmap.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 9.3 SIMILARITY HEATMAP\n# ============================================\n\ndef plot_similarity_heatmap(model, idx_to_word, title=\"Word Similarity Matrix\"):\n    \"\"\"\n    Plot a heatmap of word similarities.\n    \"\"\"\n    n_words = len(idx_to_word)\n    \n    # Compute similarity matrix\n    sim_matrix = np.zeros((n_words, n_words))\n    \n    for i in range(n_words):\n        vec_i = model.get_word_embedding(i)\n        for j in range(n_words):\n            vec_j = model.get_word_embedding(j)\n            sim_matrix[i, j] = cosine_similarity(vec_i, vec_j)\n    \n    # Create heatmap\n    fig, ax = plt.subplots(figsize=(12, 10))\n    \n    im = ax.imshow(sim_matrix, cmap='RdYlBu_r', aspect='auto', vmin=-1, vmax=1)\n    \n    # Add colorbar\n    cbar = plt.colorbar(im, ax=ax)\n    cbar.set_label('Cosine Similarity', fontsize=12)\n    \n    # Set ticks\n    words = [idx_to_word[i] for i in range(n_words)]\n    ax.set_xticks(range(n_words))\n    ax.set_yticks(range(n_words))\n    ax.set_xticklabels(words, rotation=45, ha='right', fontsize=9)\n    ax.set_yticklabels(words, fontsize=9)\n    \n    ax.set_title(title, fontsize=14, fontweight='bold')\n    ax.set_xlabel('Word', fontsize=12)\n    ax.set_ylabel('Word', fontsize=12)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Plot heatmap for Skip-Gram (Negative Sampling)\nprint(\"=\" * 60)\nprint(\"WORD SIMILARITY HEATMAP\")\nprint(\"=\" * 60)\nprint()\n\nplot_similarity_heatmap(skipgram_ns, idx_to_word, \n                        \"Skip-Gram (Neg Sampling) Word Similarities\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 10. Summary & Next Steps\n\nCongratulations! You've successfully implemented Word2Vec from scratch in PyTorch!\n\n### What We Learned\n\n| Topic | Key Points |\n|-------|-----------|\n| **Data Preparation** | Tokenization, vocabulary building, word-to-index mappings |\n| **Training Data** | Context windows, Skip-Gram pairs (center→context), CBOW pairs (context→center) |\n| **Skip-Gram** | Predicts context words from center word, works better for rare words |\n| **CBOW** | Predicts center word from context, faster training, works better for frequent words |\n| **Negative Sampling** | Efficient training by distinguishing positive vs. negative samples |\n| **Evaluation** | Cosine similarity, t-SNE visualization, word arithmetic |\n\n### Key Equations Recap\n\n**Skip-Gram Objective:**\n$$P(w_o | w_c) = \\frac{\\exp(\\vec{u}_{w_o}^T \\vec{v}_{w_c})}{\\sum_{i \\in V} \\exp(\\vec{u}_i^T \\vec{v}_{w_c})}$$\n\n**CBOW Objective:**\n$$P(w_c | w_{o_1}, ..., w_{o_{2m}}) = \\frac{\\exp(\\vec{u}_{w_c}^T \\bar{\\vec{v}}_o)}{\\sum_{i \\in V} \\exp(\\vec{u}_i^T \\bar{\\vec{v}}_o)}$$\n\nwhere $\\bar{\\vec{v}}_o$ is the average of context embeddings.\n\n**Negative Sampling Loss:**\n$$\\mathcal{L} = -\\log \\sigma(\\vec{u}_{w_o}^T \\vec{v}_{w_c}) - \\sum_{k=1}^{K} \\log \\sigma(-\\vec{u}_{w_k}^T \\vec{v}_{w_c})$$",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Next Steps\n\nNow that you understand the fundamentals, here's how to continue your learning:\n\n#### 1. Try Larger Datasets\nOur small corpus was great for learning, but real Word2Vec models train on:\n- **Text8**: 100MB Wikipedia text\n- **Wikipedia dump**: Billions of words\n- **Google News**: 100 billion words (original Word2Vec paper)\n\n```python\n# Example: Load text8 dataset\n# Download from: http://mattmahoney.net/dc/text8.zip\n```\n\n#### 2. Use Pre-trained Embeddings\nInstead of training from scratch, use embeddings trained on massive datasets:\n- **Word2Vec** (Google): 3 million words, 300 dimensions\n- **GloVe** (Stanford): Multiple sizes available\n- **FastText** (Facebook): Handles out-of-vocabulary words\n\n```python\n# Example with Gensim\nfrom gensim.models import KeyedVectors\nmodel = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n```\n\n#### 3. Explore Modern Embeddings\nWord2Vec was groundbreaking, but newer methods exist:\n- **FastText**: Uses subword information (handles rare words better)\n- **ELMo**: Contextualized embeddings\n- **BERT**: Transformer-based, context-dependent\n\n#### 4. Apply to Downstream Tasks\nUse your embeddings for:\n- **Text Classification**: Sentiment analysis, spam detection\n- **Named Entity Recognition**: Finding names, places, organizations\n- **Machine Translation**: As input features\n- **Information Retrieval**: Document similarity\n\n### References\n\n1. **Original Word2Vec Paper**: Mikolov et al., \"Efficient Estimation of Word Representations in Vector Space\" (2013)\n2. **Negative Sampling Paper**: Mikolov et al., \"Distributed Representations of Words and Phrases\" (2013)\n3. **GloVe Paper**: Pennington et al., \"GloVe: Global Vectors for Word Representation\" (2014)\n\n---\n\n**Thank you for following along! Happy learning!**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# FINAL SUMMARY\n# ============================================\n\nprint(\"=\" * 70)\nprint(\"                    NOTEBOOK COMPLETE!\")\nprint(\"=\" * 70)\nprint()\nprint(\"What you accomplished:\")\nprint(\"-\" * 70)\nprint(f\"  1. Built vocabulary from {len(corpus)} sentences ({vocab_size} unique words)\")\nprint(f\"  2. Generated {len(skipgram_pairs)} Skip-Gram training pairs\")\nprint(f\"  3. Generated {len(cbow_pairs)} CBOW training pairs\")\nprint(f\"  4. Implemented 3 models:\")\nprint(f\"     - Skip-Gram (Basic) with full softmax\")\nprint(f\"     - Skip-Gram with Negative Sampling\")\nprint(f\"     - CBOW\")\nprint(f\"  5. Trained all models for {NUM_EPOCHS} epochs\")\nprint(f\"  6. Learned {EMBEDDING_DIM}-dimensional word embeddings\")\nprint(f\"  7. Evaluated using cosine similarity and t-SNE visualization\")\nprint(f\"  8. Explored word arithmetic and similarity heatmaps\")\nprint()\nprint(\"=\" * 70)\nprint(\"                  Great job! Keep learning!\")\nprint(\"=\" * 70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}